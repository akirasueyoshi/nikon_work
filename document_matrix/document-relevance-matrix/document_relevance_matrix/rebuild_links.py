#!/usr/bin/env python3
"""
JSONãƒ•ã‚¡ã‚¤ãƒ«ã®documentséƒ¨ã‚’å…ƒã«linkséƒ¨ã‚’å†è¨ˆç®—ã—ã¦æ–°è¦JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆ
"""

import json
from pathlib import Path
from datetime import datetime
import re
import sys


def normalize_doc_name(name):
    """è³‡æ–™åã‚’æ­£è¦åŒ–ï¼ˆextract_links.pyã¨åŒã˜ãƒ­ã‚¸ãƒƒã‚¯ï¼‰"""
    # æ‹¡å¼µå­ã‚’å‰Šé™¤
    name = re.sub(r'\.(xlsx?m?|xls|docx?|pdf)$', '', name, flags=re.IGNORECASE)
    
    # æœ«å°¾ã® _æ•°å­— ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆãƒãƒ¼ã‚¸ãƒ§ãƒ³ç•ªå·ï¼‰ã‚’å‰Šé™¤
    name = re.sub(r'_\d+$', '', name)
    
    # å…¨è§’ã‚¹ãƒšãƒ¼ã‚¹ã‚’åŠè§’ã«ã€ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢ã‚’ã‚¹ãƒšãƒ¼ã‚¹ã«å¤‰æ›
    name = name.replace('ã€€', ' ').replace('_', ' ').strip()
    return name


def rebuild_links(input_json_path, output_json_path=None):
    """
    JSONãƒ•ã‚¡ã‚¤ãƒ«ã®documentséƒ¨ã‚’å…ƒã«linkséƒ¨ã‚’å†æ§‹ç¯‰
    
    Parameters:
        input_json_path: å…¥åŠ›JSONãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ï¼ˆdocumentséƒ¨ãŒæ‰‹å‹•ä¿®æ­£ã•ã‚ŒãŸã‚‚ã®ï¼‰
        output_json_path: å‡ºåŠ›JSONãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ï¼ˆçœç•¥æ™‚ã¯è‡ªå‹•ç”Ÿæˆï¼‰
    
    Returns:
        output_json_path: ç”Ÿæˆã•ã‚ŒãŸJSONãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
    """
    input_path = Path(input_json_path)
    
    if not input_path.exists():
        print(f"Error: File not found: {input_path}")
        return None
    
    # JSONãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
    print(f"Loading JSON file: {input_path}")
    with open(input_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    documents = data.get('documents', [])
    
    if not documents:
        print("Error: No documents found in JSON file")
        return None
    
    print(f"Found {len(documents)} documents")
    
    # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆIDã¨æ­£è¦åŒ–åã®ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ä½œæˆ
    doc_id_to_normalized = {}
    for doc in documents:
        doc_id = doc.get('id')
        normalized = doc.get('normalized_name', normalize_doc_name(doc.get('filename', '')))
        doc_id_to_normalized[doc_id] = normalized
        print(f"  Document: '{doc_id}' â†’ normalized: '{normalized}'")
    
    # ã™ã¹ã¦ã®æŠ½å‡ºã•ã‚ŒãŸãƒªãƒ³ã‚¯ã‚’åé›†
    all_link_texts = set()
    for doc in documents:
        extracted_links = doc.get('extracted_links', [])
        for link_text in extracted_links:
            all_link_texts.add(link_text)
    
    print(f"\nTotal unique link texts: {len(all_link_texts)}")
    
    # ãƒªãƒ³ã‚¯ãƒ†ã‚­ã‚¹ãƒˆã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ä½œæˆ
    print("\nBuilding link mapping...")
    virtual_doc_mapping = {}
    
    for link_text in all_link_texts:
        normalized = normalize_doc_name(link_text)
        
        # å®Ÿåœ¨ã™ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¨ãƒãƒƒãƒã™ã‚‹ã‹ç¢ºèª
        matched_real_doc = None
        for doc_id, doc_normalized in doc_id_to_normalized.items():
            if normalized == doc_normalized:
                matched_real_doc = doc_id
                break
        
        if matched_real_doc:
            virtual_doc_mapping[link_text] = matched_real_doc
            print(f"  âœ“ '{link_text}' â†’ Real doc: '{matched_real_doc}'")
        else:
            # ä»®æƒ³ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¨ã—ã¦æ‰±ã†
            virtual_doc_id = normalized.replace(' ', '_')
            virtual_doc_mapping[link_text] = virtual_doc_id
            print(f"  âš ï¸  '{link_text}' â†’ Virtual doc: '{virtual_doc_id}'")
    
    # ãƒªãƒ³ã‚¯ã‚’å†æ§‹ç¯‰
    print("\nRebuilding links...")
    new_links = []
    new_unmatched_links = []
    
    for doc in documents:
        source_id = doc.get('id')
        extracted_links = doc.get('extracted_links', [])
        
        for link_text in extracted_links:
            target_id = virtual_doc_mapping.get(link_text)
            
            if target_id:
                # ãƒãƒƒãƒã—ãŸå ´åˆ
                match_type = "exact" if target_id in doc_id_to_normalized else "virtual"
                new_links.append({
                    "source": source_id,
                    "target": target_id,
                    "original_text": link_text,
                    "match_type": match_type
                })
                print(f"  Link: {source_id} â†’ {target_id} (type: {match_type})")
            else:
                # ãƒãƒƒãƒã—ãªã„å ´åˆ
                new_unmatched_links.append({
                    "source": source_id,
                    "original_text": link_text,
                    "normalized": normalize_doc_name(link_text)
                })
                print(f"  Unmatched: {source_id} â†’ '{link_text}'")
    
    # çµ±è¨ˆæƒ…å ±
    matched_count = len(new_links)
    unmatched_count = len(new_unmatched_links)
    total_count = matched_count + unmatched_count
    
    print("\n" + "="*60)
    print("LINK REBUILD SUMMARY")
    print("="*60)
    print(f"Total links processed:     {total_count}")
    print(f"  - Matched links:         {matched_count}")
    print(f"  - Unmatched links:       {unmatched_count}")
    print(f"Match rate:                {matched_count/max(1, total_count)*100:.1f}%")
    print("="*60)
    
    # æ–°ã—ã„JSONãƒ‡ãƒ¼ã‚¿ã‚’æ§‹ç¯‰
    new_data = {
        "metadata": {
            "rebuild_date": datetime.now().isoformat(),
            "source_file": str(input_path),
            "original_extraction_date": data.get('metadata', {}).get('extraction_date', 'N/A'),
            "source_directory": data.get('metadata', {}).get('source_directory', 'N/A'),
            "total_documents": len(documents),
            "total_matched_links": matched_count,
            "total_unmatched_links": unmatched_count,
            "subdirectories_searched": data.get('metadata', {}).get('subdirectories_searched', 1)
        },
        "documents": documents,  # documentséƒ¨ã¯ãã®ã¾ã¾ä½¿ç”¨
        "links": new_links,
        "unmatched_links": new_unmatched_links
    }
    
    # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«åã‚’æ±ºå®š
    if output_json_path is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_json_path = input_path.parent / f"links_rebuilt_{timestamp}.json"
    else:
        output_json_path = Path(output_json_path)
    
    # JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    print(f"\nSaving rebuilt JSON to: {output_json_path}")
    with open(output_json_path, 'w', encoding='utf-8') as f:
        json.dump(new_data, f, ensure_ascii=False, indent=2)
    
    print(f"âœ“ Successfully saved: {output_json_path}")
    
    # å¤‰æ›´ç‚¹ã®ã‚µãƒãƒªãƒ¼
    if 'links' in data:
        original_matched = data['metadata'].get('total_matched_links', 0)
        original_unmatched = data['metadata'].get('total_unmatched_links', 0)
        
        print("\n" + "="*60)
        print("CHANGES FROM ORIGINAL")
        print("="*60)
        print(f"Matched links:    {original_matched} â†’ {matched_count} (Î”{matched_count - original_matched:+d})")
        print(f"Unmatched links:  {original_unmatched} â†’ {unmatched_count} (Î”{unmatched_count - original_unmatched:+d})")
        print("="*60)
    
    return output_json_path


def select_file_gui():
    """GUIã§ãƒ•ã‚¡ã‚¤ãƒ«é¸æŠãƒ€ã‚¤ã‚¢ãƒ­ã‚°ã‚’è¡¨ç¤º"""
    try:
        import tkinter as tk
        from tkinter import filedialog
        
        root = tk.Tk()
        root.withdraw()  # ãƒ¡ã‚¤ãƒ³ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚’éè¡¨ç¤º
        root.attributes('-topmost', True)  # ãƒ€ã‚¤ã‚¢ãƒ­ã‚°ã‚’æœ€å‰é¢ã«
        
        print("ãƒ•ã‚¡ã‚¤ãƒ«é¸æŠãƒ€ã‚¤ã‚¢ãƒ­ã‚°ã‚’é–‹ã„ã¦ã„ã¾ã™...")
        file_path = filedialog.askopenfilename(
            title="JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„",
            filetypes=[
                ("JSON files", "*.json"),
                ("All files", "*.*")
            ],
            initialdir="."
        )
        
        root.destroy()
        
        if file_path:
            return file_path
        else:
            print("ãƒ•ã‚¡ã‚¤ãƒ«ãŒé¸æŠã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚")
            return None
    
    except ImportError:
        print("Error: tkinter ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        print("ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã§ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚")
        return None


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã‚’ãƒã‚§ãƒƒã‚¯
    if len(sys.argv) >= 2:
        # å¼•æ•°ãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆ
        input_json = sys.argv[1]
        output_json = sys.argv[2] if len(sys.argv) > 2 else None
    else:
        # å¼•æ•°ãŒãªã„å ´åˆã¯GUIã§é¸æŠ
        print("\n" + "="*60)
        print("DOCUMENT LINKS REBUILDER - FILE SELECTION")
        print("="*60)
        print("ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        print("GUIã§ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠã—ã¾ã™...\n")
        
        input_json = select_file_gui()
        
        if input_json is None:
            print("\nUsage: python3 rebuild_links.py [input_json] [output_json]")
            print("\nDescription:")
            print("  JSONãƒ•ã‚¡ã‚¤ãƒ«ã®documentséƒ¨ã‚’å…ƒã«linkséƒ¨ã‚’å†æ§‹ç¯‰ã—ã¾ã™ã€‚")
            print("  å¼•æ•°ã‚’çœç•¥ã™ã‚‹ã¨GUIã§ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠã§ãã¾ã™ã€‚")
            print("\nExamples:")
            print("  # GUIã§ãƒ•ã‚¡ã‚¤ãƒ«é¸æŠï¼ˆå¼•æ•°ãªã—ï¼‰")
            print("  python3 rebuild_links.py")
            print("\n  # è‡ªå‹•ã§å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆ")
            print("  python3 rebuild_links.py links_extracted_20260107_123456.json")
            print("\n  # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«åã‚’æŒ‡å®š")
            print("  python3 rebuild_links.py input.json output.json")
            print("\nWorkflow:")
            print("  1. extract_links.py ã§ãƒªãƒ³ã‚¯æŠ½å‡º â†’ links_extracted_*.json")
            print("  2. JSONãƒ•ã‚¡ã‚¤ãƒ«ã®documentséƒ¨ã‚’æ‰‹å‹•ç·¨é›†ï¼ˆãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ‡ã‚£ã‚¿ã§ï¼‰")
            print("  3. ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã§linkséƒ¨ã‚’å†ç”Ÿæˆ")
            print("  4. ç”Ÿæˆã•ã‚ŒãŸJSONãƒ•ã‚¡ã‚¤ãƒ«ã§build_matrix.pyã‚’å®Ÿè¡Œ")
            return
        
        output_json = None
    
    print("\n" + "="*60)
    print("DOCUMENT LINKS REBUILDER")
    print("="*60)
    print(f"Input:  {input_json}")
    if output_json:
        print(f"Output: {output_json}")
    else:
        print(f"Output: (auto-generated)")
    print("="*60 + "\n")
    
    # ãƒªãƒ³ã‚¯å†æ§‹ç¯‰å®Ÿè¡Œ
    result_path = rebuild_links(input_json, output_json)
    
    if result_path:
        print("\n" + "="*60)
        print("âœ“ Link rebuild completed successfully!")
        print(f"Output: {result_path}")
        print("="*60)
        print("\nğŸ’¡ Next steps:")
        print(f"  1. Review the rebuilt links in: {result_path}")
        print(f"  2. Run: uv run calculate-relevance {result_path}")
    else:
        print("\nâŒ Link rebuild failed!")


if __name__ == "__main__":
    main()
