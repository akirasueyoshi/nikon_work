# Excel情報抽出スクリプトの改善提案

## 実装済みの改善

### 1. シート単位処理（extract_info_v2.py）
**問題点**: 全シートを一度にプロンプトに含めるとレート制限に到達
**解決策**: シート単位で処理し、結果を統合

**主な変更点**:
- `extract_info_from_sheet()`: 1シートずつ処理
- シート間に待機時間を挿入（`WAIT_TIME_BETWEEN_SHEETS`）
- エラーが発生しても他のシートの処理を継続
- 最終的に1つのMarkdownファイルに統合

**使い方**:
```bash
python extract_info_v2.py
```

### 2. 高度な改善版（extract_info_v3_advanced.py）
より堅牢な処理のための追加機能

**新機能**:

#### a) チャンク分割処理
- 大きなシート（デフォルト100行以上）を自動分割
- 各チャンクを個別に処理してレート制限を回避
- オプション: `--chunking`

#### b) 進捗管理
- 処理済みファイルを`progress.json`に記録
- 中断しても途中から再開可能
- 失敗したファイルも記録
- オプション: `--no-resume`で最初から実行

#### c) エラーハンドリング
- ファイル/シート/チャンク単位でのエラー処理
- エラーが発生しても処理を継続
- 詳細なログ出力

**使い方**:
```bash
# 基本的な使い方
python extract_info_v3_advanced.py

# チャンク分割を有効化
python extract_info_v3_advanced.py --chunking

# 最初から処理（進捗ファイルを無視）
python extract_info_v3_advanced.py --no-resume

# チャンク分割 + 最初から
python extract_info_v3_advanced.py --chunking --no-resume
```

## その他の改善アイデア

### 3. プロンプト最適化

#### a) 段階的な情報抽出
現在のプロンプトは一度に全カテゴリを抽出しようとしているため、複数段階に分割:

```
第1段階: 構造認識
- シートの種類を判定（仕様表、パラメータ表、フロー図など）
- 主要なセクションを識別

第2段階: 詳細抽出
- 構造に応じた適切な抽出ルールを適用
- 段階1の結果を活用して効率化
```

#### b) Few-shot学習
プロンプトに良い例と悪い例を含める:

```python
PROMPT_WITH_EXAMPLES = """
{基本プロンプト}

## 抽出例

### 良い例
入力: "関数Xは入力Aを受け取り、処理Bを実行し、結果Cを返す"
出力:
- [処理内容] 処理Bを実行 (出典: 関数仕様)
- [入出力] 入力: A、出力: C (出典: 関数仕様)

### 悪い例（推測を含む）
入力: "関数Xは入力Aを受け取る"
悪い出力: "処理Bを実行する" ← 記載されていないので不可
良い出力: [処理内容] 記載なし
"""
```

### 4. 構造化出力の活用

#### a) JSON形式での抽出
Markdownの代わりにJSON形式で抽出し、後処理を容易に:

```python
STRUCTURED_PROMPT = """
以下のJSON形式で出力してください:
{
  "sheet_name": "シート名",
  "facts": [
    {
      "category": "処理内容|入出力|状態遷移|制約・前提条件|依存・関連する機能",
      "content": "事実の内容",
      "source": "出典（セクション名/見出し/行番号）",
      "confidence": "high|medium|low"
    }
  ]
}
"""
```

メリット:
- 機械的な後処理が容易
- データベースへの格納が簡単
- 検索・フィルタリングが容易
- 信頼度スコアの追加が可能

### 5. 並列処理

#### a) ファイル単位の並列化
```python
from concurrent.futures import ThreadPoolExecutor, as_completed

def process_files_parallel(files: List[str], max_workers: int = 3):
    """
    複数ファイルを並列処理
    注意: Azure OpenAIのレート制限に注意
    """
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_to_file = {
            executor.submit(extract_info, f, INPUT_DIR, get_client()): f
            for f in files
        }
        
        for future in as_completed(future_to_file):
            file = future_to_file[future]
            try:
                result = future.result()
                save_result(file, result)
            except Exception as e:
                log_error(file, e)
```

注意点:
- Azure OpenAIのレート制限を考慮してmax_workersを調整
- 各スレッドで別々のクライアントを使用

### 6. キャッシング戦略

#### a) 中間結果のキャッシュ
```python
import hashlib
import pickle

def get_cache_key(sheet_name: str, df: pd.DataFrame) -> str:
    """DataFrameの内容からハッシュを生成"""
    content = f"{sheet_name}_{df.to_csv()}"
    return hashlib.md5(content.encode()).hexdigest()

def extract_with_cache(sheet_name: str, df: pd.DataFrame, cache_dir: str = "cache"):
    cache_key = get_cache_key(sheet_name, df)
    cache_path = os.path.join(cache_dir, f"{cache_key}.pkl")
    
    # キャッシュがあれば利用
    if os.path.exists(cache_path):
        with open(cache_path, 'rb') as f:
            return pickle.load(f)
    
    # 新規処理
    result = extract_info_from_sheet(...)
    
    # キャッシュに保存
    os.makedirs(cache_dir, exist_ok=True)
    with open(cache_path, 'wb') as f:
        pickle.dump(result, f)
    
    return result
```

### 7. 品質チェック機構

#### a) 抽出結果の検証
```python
def validate_extraction(extracted_info: str, original_df: pd.DataFrame) -> Dict:
    """
    抽出結果の品質をチェック
    """
    validation = {
        "has_content": len(extracted_info.strip()) > 0,
        "has_facts": "記載なし" not in extracted_info or "[" in extracted_info,
        "has_sources": "出典:" in extracted_info,
        "empty_categories": []
    }
    
    # 各カテゴリが空でないかチェック
    categories = ["処理内容", "入出力", "状態遷移", "制約・前提条件", "依存・関連する機能"]
    for cat in categories:
        if f"[{cat}]" in extracted_info:
            # カテゴリの内容を抽出して確認
            pattern = f"\\[{cat}\\]\\s*記載なし"
            if re.search(pattern, extracted_info):
                validation["empty_categories"].append(cat)
    
    return validation
```

#### b) 自動レビュー
```python
def auto_review(extracted_info: str, client: AzureOpenAI) -> str:
    """
    別のLLM呼び出しで抽出結果をレビュー
    """
    review_prompt = """
    以下の抽出結果をレビューし、以下の観点でチェックしてください:
    1. 推測や一般知識が含まれていないか
    2. 原文の語彙が維持されているか
    3. 出典が適切に記載されているか
    4. 改善提案
    
    抽出結果:
    {extracted_info}
    """
    
    response = client.chat.completions.create(
        model=DEPLOYMENT_NAME,
        messages=[
            {"role": "system", "content": "あなたは技術文書レビューの専門家です"},
            {"role": "user", "content": review_prompt.format(extracted_info=extracted_info)}
        ]
    )
    return response.choices[0].message.content
```

### 8. バッチAPIの活用（推奨）

Azure OpenAIのBatch APIを使用してコスト削減とレート制限回避:

```python
def create_batch_requests(files: List[str]) -> List[Dict]:
    """
    バッチAPI用のリクエストを作成
    """
    requests = []
    for file in files:
        sheets = pd.read_excel(file, sheet_name=None)
        for sheet_name, df in sheets.items():
            requests.append({
                "custom_id": f"{file}_{sheet_name}",
                "method": "POST",
                "url": "/chat/completions",
                "body": {
                    "model": DEPLOYMENT_NAME,
                    "messages": [
                        {"role": "system", "content": PROMPT},
                        {"role": "user", "content": df.to_markdown()}
                    ],
                    "max_tokens": 10000
                }
            })
    return requests

# バッチファイルを作成して送信
# 処理完了まで24時間程度かかるが、コストは50%削減
```

### 9. インクリメンタル処理

#### a) 差分更新
```python
def get_file_hash(filepath: str) -> str:
    """ファイルのハッシュを計算"""
    with open(filepath, 'rb') as f:
        return hashlib.md5(f.read()).hexdigest()

def needs_reprocessing(filepath: str, hash_db: Dict) -> bool:
    """
    ファイルが更新されているかチェック
    """
    current_hash = get_file_hash(filepath)
    stored_hash = hash_db.get(filepath)
    return current_hash != stored_hash

# 変更されたファイルのみ再処理
```

### 10. ストリーミング処理

長時間の処理でフィードバックを得る:

```python
def extract_with_streaming(client: AzureOpenAI, messages: List[Dict]) -> str:
    """
    ストリーミングモードで抽出し、進捗を表示
    """
    stream = client.chat.completions.create(
        model=DEPLOYMENT_NAME,
        messages=messages,
        stream=True
    )
    
    full_response = []
    for chunk in stream:
        if chunk.choices[0].delta.content:
            content = chunk.choices[0].delta.content
            full_response.append(content)
            print(content, end='', flush=True)
    
    print()  # 改行
    return ''.join(full_response)
```

## 推奨実装順序

1. **即座に実装**: extract_info_v2.py（シート単位処理）
2. **短期**: 進捗管理とエラーハンドリング（v3に実装済み）
3. **中期**: 構造化出力（JSON形式）、品質チェック
4. **長期**: バッチAPI、並列処理、キャッシング

## パフォーマンス比較

| 方式 | 処理速度 | コスト | レート制限への強さ | 実装難易度 |
|------|---------|--------|------------------|-----------|
| 元の実装 | 速い | 低 | 弱い | 易 |
| v2 (シート単位) | 中 | 低 | 中 | 易 |
| v3 (チャンク分割) | 遅い | 中 | 強い | 中 |
| バッチAPI | 非常に遅い | 非常に低 | 非常に強い | 中 |
| 並列処理 | 速い | 中 | 要調整 | 難 |

## まとめ

**今すぐ使える解決策**:
- `extract_info_v2.py`: シンプルで効果的
- `extract_info_v3_advanced.py`: より堅牢、大規模データ対応

**次のステップ**:
1. JSON形式の構造化出力を検討
2. プロンプトエンジニアリングの改善
3. 必要に応じてバッチAPIの検討
