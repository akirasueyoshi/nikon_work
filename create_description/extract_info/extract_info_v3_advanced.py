import pandas as pd
import os
import time
import json
from typing import List, Dict, Tuple
from azure.identity import InteractiveBrowserCredential, get_bearer_token_provider
from openai import AzureOpenAI

AZURE_OPENAI_ENDPOINT = 'https://aoai-je-exm.openai.azure.com/'
DEPLOYMENT_NAME = 'gpt-4o'

PROMPT = """
ä»¥é™ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§ç¤ºã•ã‚Œã‚‹ãƒ†ã‚­ã‚¹ãƒˆã¯Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’DataframeåŒ–ã—ãŸã‚‚ã®ã§ã™ã€‚
ã“ã®Excelãƒ•ã‚¡ã‚¤ãƒ«ã¯åŠå°ä½“éœ²å…‰è£…ç½®ã«ãŠã‘ã‚‹{basename}ã¨ã„ã†æ©Ÿèƒ½ä»•æ§˜ã‚’è¡¨ç¾ã—ãŸä»•æ§˜æ›¸ã§ã™ã€‚
ã“ã®ä»•æ§˜æ›¸ã®è¨˜è¿°ã‹ã‚‰ã€Œæ˜ç¤ºçš„ã«è¨˜è¼‰ã•ã‚Œã¦ã„ã‚‹ã€äº‹å®Ÿã®ã¿ã‚’ä»¥ä¸‹ã®æŒ‡ç¤ºã«å¾“ã£ã¦æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚
å›ç­”ã¯Markdownå½¢å¼ã®è¨˜è¿°ã¨ã—ã¦ãã ã•ã„ã€‚

## æŠ½å‡ºã™ã‚‹äº‹å®Ÿã®ç¨®é¡
- å‡¦ç†å†…å®¹ï¼ˆä½•ã‚’ã™ã‚‹ã‹ï¼‰
- å…¥å‡ºåŠ›ï¼ˆä½•ã‚’å—ã‘å–ã‚Šä½•ã‚’è¿”ã™ã‹ï¼‰
- çŠ¶æ…‹é·ç§»
- åˆ¶ç´„ãƒ»å‰ææ¡ä»¶
- ä¾å­˜ãƒ»é–¢é€£ã™ã‚‹æ©Ÿèƒ½

## æŠ½å‡ºãƒ«ãƒ¼ãƒ«
- æ¨æ¸¬ãƒ»ä¸€èˆ¬çŸ¥è­˜ãƒ»è¨€ã„æ›ãˆã¯ç¦æ­¢
- åŸæ–‡ã®èªå½™ã‚’ã§ãã‚‹ã ã‘ç¶­æŒã™ã‚‹ã“ã¨
- æ›¸ã‹ã‚Œã¦ã„ãªã„ã“ã¨ã¯ã€Œè¨˜è¼‰ãªã—ã€ã¨ã—ã€è£œå®Œã—ãªã„
- 1ã¤ã®äº‹å®Ÿ = 1ã¤ã®ç‹¬ç«‹ã—ãŸä¸»å¼µ(1ã€œ3æ–‡ç¨‹åº¦)ã¨ã™ã‚‹
- å‡ºå…¸ï¼ˆã‚»ã‚¯ã‚·ãƒ§ãƒ³å/è¦‹å‡ºã—ï¼‰ã‚’ä»˜ä¸ã™ã‚‹ã“ã¨

## å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
- [å‡¦ç†å†…å®¹] 
- [å…¥å‡ºåŠ›] 
- [çŠ¶æ…‹é·ç§»] 
- [åˆ¶ç´„ãƒ»å‰ææ¡ä»¶] 
- [ä¾å­˜ãƒ»é–¢é€£ã™ã‚‹æ©Ÿèƒ½] 

"""

INPUT_DIR = r'resource'
OUTPUT_DIR = r'information'
PROGRESS_FILE = r'progress.json'

# ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–ã®å¾…æ©Ÿæ™‚é–“ï¼ˆç§’ï¼‰
WAIT_TIME_BETWEEN_SHEETS = 2
WAIT_TIME_BETWEEN_FILES = 5

# ã‚·ãƒ¼ãƒˆã‚’åˆ†å‰²ã™ã‚‹é–¾å€¤ï¼ˆè¡Œæ•°ï¼‰
MAX_ROWS_PER_CHUNK = 100


def get_excel_files(dir_path: str) -> list[str]:
    """
    æŒ‡å®šãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªé…ä¸‹ã®ã™ã¹ã¦ã®Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’å†å¸°çš„ã«å–å¾—
    ãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰ã®ç›¸å¯¾ãƒ‘ã‚¹ï¼ˆæ‹¡å¼µå­ãªã—ï¼‰ã®ãƒªã‚¹ãƒˆã‚’è¿”ã™
    """
    excel_files = []
    
    if not os.path.exists(dir_path):
        os.makedirs(dir_path)
        return excel_files
    
    for root, dirs, files in os.walk(dir_path):
        for file in files:
            if file.endswith(('.xlsx', '.xls')):
                full_path = os.path.join(root, file)
                rel_path = os.path.relpath(full_path, dir_path)
                rel_path_without_ext = os.path.splitext(rel_path)[0]
                excel_files.append(rel_path_without_ext)
    
    return excel_files


def get_client() -> AzureOpenAI:
    credential = InteractiveBrowserCredential(
        tenant_id="4876a51c-4f2d-4d54-b712-e0b67d308e80"
    )
    
    token_provider = get_bearer_token_provider(
        credential,
        "https://cognitiveservices.azure.com/.default"
    )

    return AzureOpenAI(
        azure_endpoint=AZURE_OPENAI_ENDPOINT,
        azure_ad_token_provider=token_provider,
        api_version="2024-12-01-preview",
    )


def split_dataframe_into_chunks(df: pd.DataFrame, max_rows: int = MAX_ROWS_PER_CHUNK) -> List[Tuple[int, int, pd.DataFrame]]:
    """
    DataFrameã‚’æŒ‡å®šè¡Œæ•°ã§åˆ†å‰²
    
    Args:
        df: åˆ†å‰²ã™ã‚‹DataFrame
        max_rows: ãƒãƒ£ãƒ³ã‚¯ã‚ãŸã‚Šã®æœ€å¤§è¡Œæ•°
    
    Returns:
        (é–‹å§‹è¡Œ, çµ‚äº†è¡Œ, ãƒãƒ£ãƒ³ã‚¯)ã®ãƒªã‚¹ãƒˆ
    """
    chunks = []
    total_rows = len(df)
    
    for start in range(0, total_rows, max_rows):
        end = min(start + max_rows, total_rows)
        chunk = df.iloc[start:end]
        chunks.append((start, end, chunk))
    
    return chunks


def extract_info_from_chunk(basename: str, sheet_name: str, chunk_info: str, 
                           chunk_df: pd.DataFrame, client: AzureOpenAI) -> str:
    """
    1ã¤ã®ãƒãƒ£ãƒ³ã‚¯ã‹ã‚‰æƒ…å ±ã‚’æŠ½å‡º
    
    Args:
        basename: ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ™ãƒ¼ã‚¹å
        sheet_name: ã‚·ãƒ¼ãƒˆå
        chunk_info: ãƒãƒ£ãƒ³ã‚¯æƒ…å ±ï¼ˆä¾‹: "è¡Œ1-100"ï¼‰
        chunk_df: ãƒãƒ£ãƒ³ã‚¯ã®DataFrame
        client: AzureOpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
    
    Returns:
        æŠ½å‡ºã•ã‚ŒãŸæƒ…å ±ï¼ˆMarkdownå½¢å¼ï¼‰
    """
    table_text = f"## ã‚·ãƒ¼ãƒˆ: {sheet_name} ({chunk_info})\n\n{chunk_df.to_markdown()}"
    
    response = client.chat.completions.create(
        model=DEPLOYMENT_NAME,
        messages=[
            {"role": "system", "content": PROMPT.format(basename=basename)},
            {"role": "user", "content": table_text}
        ],
        max_tokens=10000
    )
    return response.choices[0].message.content


def extract_info_from_sheet(basename: str, sheet_name: str, df: pd.DataFrame, 
                           client: AzureOpenAI, use_chunking: bool = False) -> str:
    """
    1ã¤ã®ã‚·ãƒ¼ãƒˆã‹ã‚‰æƒ…å ±ã‚’æŠ½å‡ºï¼ˆå¿…è¦ã«å¿œã˜ã¦ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ï¼‰
    
    Args:
        basename: ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ™ãƒ¼ã‚¹å
        sheet_name: ã‚·ãƒ¼ãƒˆå
        df: ã‚·ãƒ¼ãƒˆã®DataFrame
        client: AzureOpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
        use_chunking: ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ã‚’ä½¿ç”¨ã™ã‚‹ã‹
    
    Returns:
        æŠ½å‡ºã•ã‚ŒãŸæƒ…å ±ï¼ˆMarkdownå½¢å¼ï¼‰
    """
    if not use_chunking or len(df) <= MAX_ROWS_PER_CHUNK:
        # ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ä¸è¦
        table_text = f"## ã‚·ãƒ¼ãƒˆ: {sheet_name}\n\n{df.to_markdown()}"
        
        response = client.chat.completions.create(
            model=DEPLOYMENT_NAME,
            messages=[
                {"role": "system", "content": PROMPT.format(basename=basename)},
                {"role": "user", "content": table_text}
            ],
            max_tokens=10000
        )
        return response.choices[0].message.content
    else:
        # ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ã—ã¦å‡¦ç†
        chunks = split_dataframe_into_chunks(df)
        chunk_results = []
        
        print(f'    Splitting into {len(chunks)} chunks...')
        
        for i, (start, end, chunk_df) in enumerate(chunks, 1):
            chunk_info = f"è¡Œ{start+1}-{end}"
            print(f'    Processing chunk {i}/{len(chunks)}: {chunk_info}')
            
            try:
                chunk_result = extract_info_from_chunk(
                    basename, sheet_name, chunk_info, chunk_df, client
                )
                chunk_results.append(f"### {chunk_info}\n\n{chunk_result}")
                
                if i < len(chunks):
                    time.sleep(WAIT_TIME_BETWEEN_SHEETS)
                    
            except Exception as e:
                print(f'    âš  Error processing chunk {chunk_info}: {str(e)}')
                chunk_results.append(f"### {chunk_info}\n\n**ã‚¨ãƒ©ãƒ¼**: {str(e)}")
        
        return "\n\n".join(chunk_results)


def extract_info(rel_path: str, resource_dir: str, client: AzureOpenAI, 
                use_chunking: bool = False) -> str:
    """
    Excelãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æƒ…å ±ã‚’æŠ½å‡ºï¼ˆã‚·ãƒ¼ãƒˆå˜ä½ã§å‡¦ç†ï¼‰
    
    Args:
        rel_path: resourceãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰ã®ç›¸å¯¾ãƒ‘ã‚¹ï¼ˆæ‹¡å¼µå­ãªã—ï¼‰
        resource_dir: resourceãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹
        client: AzureOpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
        use_chunking: ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ã‚’ä½¿ç”¨ã™ã‚‹ã‹
    
    Returns:
        å…¨ã‚·ãƒ¼ãƒˆã‹ã‚‰æŠ½å‡ºã•ã‚ŒãŸæƒ…å ±ã‚’çµ±åˆã—ãŸMarkdownæ–‡å­—åˆ—
    """
    basename = os.path.basename(rel_path)
    
    excel_path = os.path.join(resource_dir, f'{rel_path}.xlsx')
    if not os.path.exists(excel_path):
        excel_path = os.path.join(resource_dir, f'{rel_path}.xls')
    
    all_sheets = pd.read_excel(excel_path, sheet_name=None)
    
    extracted_parts = [f"# {basename} - æŠ½å‡ºæƒ…å ±\n\n"]
    
    for i, (sheet_name, df) in enumerate(all_sheets.items(), 1):
        print(f'  Processing sheet {i}/{len(all_sheets)}: {sheet_name} ({len(df)} rows)')
        
        try:
            sheet_info = extract_info_from_sheet(basename, sheet_name, df, client, use_chunking)
            
            extracted_parts.append(f"## ã‚·ãƒ¼ãƒˆ: {sheet_name}\n\n")
            extracted_parts.append(sheet_info)
            extracted_parts.append("\n\n---\n\n")
            
            if i < len(all_sheets):
                time.sleep(WAIT_TIME_BETWEEN_SHEETS)
            
        except Exception as e:
            print(f'  âš  Error processing sheet {sheet_name}: {str(e)}')
            extracted_parts.append(f"## ã‚·ãƒ¼ãƒˆ: {sheet_name}\n\n")
            extracted_parts.append(f"**ã‚¨ãƒ©ãƒ¼**: {str(e)}\n\n---\n\n")
    
    return "".join(extracted_parts)


def load_progress() -> Dict:
    """å‡¦ç†é€²æ—ã‚’èª­ã¿è¾¼ã¿"""
    if os.path.exists(PROGRESS_FILE):
        with open(PROGRESS_FILE, 'r', encoding='utf-8') as f:
            return json.load(f)
    return {"completed": [], "failed": []}


def save_progress(progress: Dict):
    """å‡¦ç†é€²æ—ã‚’ä¿å­˜"""
    with open(PROGRESS_FILE, 'w', encoding='utf-8') as f:
        json.dump(progress, f, ensure_ascii=False, indent=2)


def main(use_chunking: bool = False, resume: bool = True):
    """
    ãƒ¡ã‚¤ãƒ³å‡¦ç†
    
    Args:
        use_chunking: å¤§ããªã‚·ãƒ¼ãƒˆã‚’ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ã™ã‚‹ã‹
        resume: ä¸­æ–­ã—ãŸã¨ã“ã‚ã‹ã‚‰å†é–‹ã™ã‚‹ã‹
    """
    inputs = get_excel_files(INPUT_DIR)
    
    # é€²æ—æƒ…å ±ã‚’èª­ã¿è¾¼ã¿
    progress = load_progress() if resume else {"completed": [], "failed": []}
    
    # å‡ºåŠ›æ¸ˆã¿ã®ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—
    output_mds = []
    if os.path.exists(OUTPUT_DIR):
        for root, dirs, files in os.walk(OUTPUT_DIR):
            for file in files:
                if file.endswith('.md'):
                    full_path = os.path.join(root, file)
                    rel_path = os.path.relpath(full_path, OUTPUT_DIR)
                    rel_path_without_ext = os.path.splitext(rel_path)[0]
                    output_mds.append(rel_path_without_ext)
    
    # æœªå‡¦ç†ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æŠ½å‡ºï¼ˆå®Œäº†æ¸ˆã¿ã¨å¤±æ•—æ¸ˆã¿ã‚’é™¤å¤–ï¼‰
    completed_set = set(progress.get("completed", []))
    undescribeds = [f for f in inputs if f not in output_mds and f not in completed_set]
    
    print(f'Found {len(inputs)} Excel files')
    print(f'Already completed: {len(progress.get("completed", []))}')
    print(f'Previously failed: {len(progress.get("failed", []))}')
    print(f'Processing {len(undescribeds)} files...')
    print(f'Chunking mode: {"ON" if use_chunking else "OFF"}')
    
    client = get_client()
    
    for idx, undescribed in enumerate(undescribeds, 1):
        try:
            print(f'\n[{idx}/{len(undescribeds)}] ğŸ“„ Processing: {undescribed}')
            description = extract_info(undescribed, INPUT_DIR, client, use_chunking)
            
            output_path = os.path.join(OUTPUT_DIR, f'{undescribed}.md')
            output_dir = os.path.dirname(output_path)
            if output_dir and not os.path.exists(output_dir):
                os.makedirs(output_dir, exist_ok=True)
            
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(description)
            
            # é€²æ—ã‚’æ›´æ–°
            progress["completed"].append(undescribed)
            if undescribed in progress.get("failed", []):
                progress["failed"].remove(undescribed)
            save_progress(progress)
            
            print(f'âœ“ Information file created: {undescribed}')
            
            # ãƒ•ã‚¡ã‚¤ãƒ«é–“ã®å¾…æ©Ÿ
            if idx < len(undescribeds):
                time.sleep(WAIT_TIME_BETWEEN_FILES)
            
        except Exception as e:
            print(f'âœ— Error processing {undescribed}: {str(e)}')
            if undescribed not in progress.get("failed", []):
                progress.setdefault("failed", []).append(undescribed)
            save_progress(progress)
    
    print(f'\n{"="*60}')
    print(f'Processing complete!')
    print(f'Total completed: {len(progress.get("completed", []))}')
    print(f'Total failed: {len(progress.get("failed", []))}')


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='Extract information from Excel files')
    parser.add_argument('--chunking', action='store_true', 
                       help='Enable chunking for large sheets')
    parser.add_argument('--no-resume', action='store_true',
                       help='Start from scratch (ignore progress file)')
    
    args = parser.parse_args()
    
    main(use_chunking=args.chunking, resume=not args.no_resume)
