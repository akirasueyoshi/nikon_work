import pandas as pd
import os
import time
import openpyxl
from PIL import Image
import io
import base64
from typing import List, Tuple, Dict, Optional
from azure.identity import InteractiveBrowserCredential, get_bearer_token_provider
from openai import AzureOpenAI

AZURE_OPENAI_ENDPOINT = 'https://aoai-je-exm.openai.azure.com/'
DEPLOYMENT_NAME = 'gpt-4o'  # gpt-4oã¯visionå¯¾å¿œ

PROMPT = """
ä»¥é™ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§ç¤ºã•ã‚Œã‚‹ãƒ†ã‚­ã‚¹ãƒˆã¯Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’DataframeåŒ–ã—ãŸã‚‚ã®ã§ã™ã€‚
ã“ã®Excelãƒ•ã‚¡ã‚¤ãƒ«ã¯åŠå°ä½“éœ²å…‰è£…ç½®ã«ãŠã‘ã‚‹{basename}ã¨ã„ã†æ©Ÿèƒ½ä»•æ§˜ã‚’è¡¨ç¾ã—ãŸä»•æ§˜æ›¸ã§ã™ã€‚
ã“ã®ä»•æ§˜æ›¸ã®è¨˜è¿°ã‹ã‚‰ã€Œæ˜ç¤ºçš„ã«è¨˜è¼‰ã•ã‚Œã¦ã„ã‚‹ã€äº‹å®Ÿã®ã¿ã‚’ä»¥ä¸‹ã®æŒ‡ç¤ºã«å¾“ã£ã¦æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚
å›ç­”ã¯Markdownå½¢å¼ã®è¨˜è¿°ã¨ã—ã¦ãã ã•ã„ã€‚

## æŠ½å‡ºã™ã‚‹äº‹å®Ÿã®ç¨®é¡
- å‡¦ç†å†…å®¹ï¼ˆä½•ã‚’ã™ã‚‹ã‹ï¼‰
- å…¥å‡ºåŠ›ï¼ˆä½•ã‚’å—ã‘å–ã‚Šä½•ã‚’è¿”ã™ã‹ï¼‰
- çŠ¶æ…‹é·ç§»
- åˆ¶ç´„ãƒ»å‰ææ¡ä»¶
- ä¾å­˜ãƒ»é–¢é€£ã™ã‚‹æ©Ÿèƒ½

## æŠ½å‡ºãƒ«ãƒ¼ãƒ«
- æ¨æ¸¬ãƒ»ä¸€èˆ¬çŸ¥è­˜ãƒ»è¨€ã„æ›ãˆã¯ç¦æ­¢
- åŸæ–‡ã®èªå½™ã‚’ã§ãã‚‹ã ã‘ç¶­æŒã™ã‚‹ã“ã¨
- æ›¸ã‹ã‚Œã¦ã„ãªã„ã“ã¨ã¯ã€Œè¨˜è¼‰ãªã—ã€ã¨ã—ã€è£œå®Œã—ãªã„
- 1ã¤ã®äº‹å®Ÿ = 1ã¤ã®ç‹¬ç«‹ã—ãŸä¸»å¼µ(1ã€œ3æ–‡ç¨‹åº¦)ã¨ã™ã‚‹
- å‡ºå…¸ï¼ˆã‚»ã‚¯ã‚·ãƒ§ãƒ³å/è¦‹å‡ºã—ï¼‰ã‚’ä»˜ä¸ã™ã‚‹ã“ã¨

## ç”»åƒã«ã¤ã„ã¦
- ç”»åƒãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã¯ã€ç”»åƒã®å†…å®¹ã‚‚åˆ†æã—ã¦æƒ…å ±ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„
- å›³è¡¨ã€ãƒ•ãƒ­ãƒ¼ãƒãƒ£ãƒ¼ãƒˆã€çŠ¶æ…‹é·ç§»å›³ãªã©ã¯ç‰¹ã«é‡è¦ã§ã™
- ç”»åƒã‹ã‚‰èª­ã¿å–ã‚Œã‚‹æƒ…å ±ã‚‚ä¸Šè¨˜ã®äº‹å®Ÿã¨ã—ã¦æŠ½å‡ºã—ã¦ãã ã•ã„

## å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
- [å‡¦ç†å†…å®¹] 
- [å…¥å‡ºåŠ›] 
- [çŠ¶æ…‹é·ç§»] 
- [åˆ¶ç´„ãƒ»å‰ææ¡ä»¶] 
- [ä¾å­˜ãƒ»é–¢é€£ã™ã‚‹æ©Ÿèƒ½] 

"""

INPUT_DIR = r'resource'
OUTPUT_DIR = r'information'

# ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–ã®å¾…æ©Ÿæ™‚é–“ï¼ˆç§’ï¼‰
WAIT_TIME_BETWEEN_SHEETS = 2
WAIT_TIME_BETWEEN_FILES = 5

# ã‚·ãƒ¼ãƒˆã‚’åˆ†å‰²ã™ã‚‹é–¾å€¤ï¼ˆè¡Œæ•°ï¼‰
MAX_ROWS_PER_CHUNK = 100


def get_excel_files(dir_path: str) -> list[str]:
    """
    æŒ‡å®šãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªé…ä¸‹ã®ã™ã¹ã¦ã®Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’å†å¸°çš„ã«å–å¾—
    ãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰ã®ç›¸å¯¾ãƒ‘ã‚¹ï¼ˆæ‹¡å¼µå­ãªã—ï¼‰ã®ãƒªã‚¹ãƒˆã‚’è¿”ã™
    """
    excel_files = []
    
    if not os.path.exists(dir_path):
        os.makedirs(dir_path)
        return excel_files
    
    for root, dirs, files in os.walk(dir_path):
        for file in files:
            if file.endswith(('.xlsx', '.xls')):
                full_path = os.path.join(root, file)
                rel_path = os.path.relpath(full_path, dir_path)
                rel_path_without_ext = os.path.splitext(rel_path)[0]
                excel_files.append(rel_path_without_ext)
    
    return excel_files


def get_client() -> AzureOpenAI:
    credential = InteractiveBrowserCredential(
        tenant_id="4876a51c-4f2d-4d54-b712-e0b67d308e80"
    )
    
    token_provider = get_bearer_token_provider(
        credential,
        "https://cognitiveservices.azure.com/.default"
    )

    return AzureOpenAI(
        azure_endpoint=AZURE_OPENAI_ENDPOINT,
        azure_ad_token_provider=token_provider,
        api_version="2024-12-01-preview",
    )


def extract_images_from_sheet(excel_path: str, sheet_name: str) -> List[Image.Image]:
    """
    ç‰¹å®šã®ã‚·ãƒ¼ãƒˆã‹ã‚‰ç”»åƒã‚’æŠ½å‡º
    
    Args:
        excel_path: Excelãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        sheet_name: ã‚·ãƒ¼ãƒˆå
    
    Returns:
        PIL Imageã®ãƒªã‚¹ãƒˆ
    """
    try:
        wb = openpyxl.load_workbook(excel_path)
        sheet = wb[sheet_name]
        
        images = []
        if hasattr(sheet, '_images'):
            for img in sheet._images:
                image_data = img._data()
                pil_image = Image.open(io.BytesIO(image_data))
                images.append(pil_image)
        
        return images
    except Exception as e:
        print(f'    Warning: Could not extract images from {sheet_name}: {str(e)}')
        return []


def image_to_base64(image: Image.Image) -> str:
    """
    PIL Imageã‚’Base64æ–‡å­—åˆ—ã«å¤‰æ›
    
    Args:
        image: PIL Image
    
    Returns:
        Base64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã•ã‚ŒãŸæ–‡å­—åˆ—
    """
    buffered = io.BytesIO()
    # å¤§ããªç”»åƒã¯ãƒªã‚µã‚¤ã‚ºï¼ˆãƒˆãƒ¼ã‚¯ãƒ³ç¯€ç´„ï¼‰
    max_size = (1024, 1024)
    image.thumbnail(max_size, Image.Resampling.LANCZOS)
    image.save(buffered, format="PNG")
    img_str = base64.b64encode(buffered.getvalue()).decode()
    return img_str


def create_message_content(text: str, images: List[Image.Image]) -> list:
    """
    ãƒ†ã‚­ã‚¹ãƒˆã¨ç”»åƒã‚’å«ã‚€ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ä½œæˆ
    
    Args:
        text: ãƒ†ã‚­ã‚¹ãƒˆå†…å®¹
        images: PIL Imageã®ãƒªã‚¹ãƒˆ
    
    Returns:
        Azure OpenAI APIã«æ¸¡ã›ã‚‹ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å½¢å¼
    """
    content = [
        {
            "type": "text",
            "text": text
        }
    ]
    
    # ç”»åƒã‚’è¿½åŠ ï¼ˆæœ€å¤§5æšã¾ã§ï¼‰
    for img in images[:5]:
        base64_image = image_to_base64(img)
        content.append({
            "type": "image_url",
            "image_url": {
                "url": f"data:image/png;base64,{base64_image}"
            }
        })
    
    return content


def split_dataframe_into_chunks(df: pd.DataFrame, max_rows: int = MAX_ROWS_PER_CHUNK) -> List[Tuple[int, int, pd.DataFrame]]:
    """
    DataFrameã‚’æŒ‡å®šè¡Œæ•°ã§åˆ†å‰²
    
    Args:
        df: åˆ†å‰²ã™ã‚‹DataFrame
        max_rows: ãƒãƒ£ãƒ³ã‚¯ã‚ãŸã‚Šã®æœ€å¤§è¡Œæ•°
    
    Returns:
        (é–‹å§‹è¡Œ, çµ‚äº†è¡Œ, ãƒãƒ£ãƒ³ã‚¯)ã®ãƒªã‚¹ãƒˆ
    """
    chunks = []
    total_rows = len(df)
    
    for start in range(0, total_rows, max_rows):
        end = min(start + max_rows, total_rows)
        chunk = df.iloc[start:end]
        chunks.append((start, end, chunk))
    
    return chunks


def extract_info_from_sheet(basename: str, sheet_name: str, df: pd.DataFrame, 
                           excel_path: str, client: AzureOpenAI, 
                           use_chunking: bool = False,
                           include_images: bool = True) -> str:
    """
    1ã¤ã®ã‚·ãƒ¼ãƒˆã‹ã‚‰æƒ…å ±ã‚’æŠ½å‡ºï¼ˆç”»åƒå¯¾å¿œç‰ˆï¼‰
    
    Args:
        basename: ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ™ãƒ¼ã‚¹å
        sheet_name: ã‚·ãƒ¼ãƒˆå
        df: ã‚·ãƒ¼ãƒˆã®DataFrame
        excel_path: Excelãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ï¼ˆç”»åƒæŠ½å‡ºç”¨ï¼‰
        client: AzureOpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
        use_chunking: ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ã‚’ä½¿ç”¨ã™ã‚‹ã‹
        include_images: ç”»åƒã‚’å«ã‚ã‚‹ã‹
    
    Returns:
        æŠ½å‡ºã•ã‚ŒãŸæƒ…å ±ï¼ˆMarkdownå½¢å¼ï¼‰
    """
    # ç”»åƒã‚’æŠ½å‡º
    images = []
    if include_images:
        images = extract_images_from_sheet(excel_path, sheet_name)
        if images:
            print(f'    Found {len(images)} images in sheet')
    
    # ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ãŒä¸è¦ãªå ´åˆ
    if not use_chunking or len(df) <= MAX_ROWS_PER_CHUNK:
        table_text = f"## ã‚·ãƒ¼ãƒˆ: {sheet_name}\n\n{df.to_markdown()}"
        
        # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ä½œæˆï¼ˆç”»åƒå«ã‚€ï¼‰
        user_content = create_message_content(table_text, images)
        
        response = client.chat.completions.create(
            model=DEPLOYMENT_NAME,
            messages=[
                {"role": "system", "content": PROMPT.format(basename=basename)},
                {"role": "user", "content": user_content}
            ],
            max_tokens=10000
        )
        return response.choices[0].message.content
    
    # ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ã—ã¦å‡¦ç†
    chunks = split_dataframe_into_chunks(df)
    chunk_results = []
    
    print(f'    Splitting into {len(chunks)} chunks...')
    
    for i, (start, end, chunk_df) in enumerate(chunks, 1):
        chunk_info = f"è¡Œ{start+1}-{end}"
        print(f'    Processing chunk {i}/{len(chunks)}: {chunk_info}')
        
        try:
            table_text = f"## ã‚·ãƒ¼ãƒˆ: {sheet_name} ({chunk_info})\n\n{chunk_df.to_markdown()}"
            
            # æœ€åˆã®ãƒãƒ£ãƒ³ã‚¯ã«ã®ã¿ç”»åƒã‚’å«ã‚ã‚‹ï¼ˆå…¨ãƒãƒ£ãƒ³ã‚¯ã«å«ã‚ã‚‹ã¨ãƒˆãƒ¼ã‚¯ãƒ³è¶…éã®æã‚Œï¼‰
            chunk_images = images if i == 1 else []
            user_content = create_message_content(table_text, chunk_images)
            
            response = client.chat.completions.create(
                model=DEPLOYMENT_NAME,
                messages=[
                    {"role": "system", "content": PROMPT.format(basename=basename)},
                    {"role": "user", "content": user_content}
                ],
                max_tokens=10000
            )
            chunk_results.append(f"### {chunk_info}\n\n{response.choices[0].message.content}")
            
            if i < len(chunks):
                time.sleep(WAIT_TIME_BETWEEN_SHEETS)
                
        except Exception as e:
            print(f'    âš  Error processing chunk {chunk_info}: {str(e)}')
            chunk_results.append(f"### {chunk_info}\n\n**ã‚¨ãƒ©ãƒ¼**: {str(e)}")
    
    return "\n\n".join(chunk_results)


def extract_info(rel_path: str, resource_dir: str, client: AzureOpenAI, 
                use_chunking: bool = False, include_images: bool = True) -> str:
    """
    Excelãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æƒ…å ±ã‚’æŠ½å‡ºï¼ˆã‚·ãƒ¼ãƒˆå˜ä½ã§å‡¦ç†ã€ç”»åƒå¯¾å¿œï¼‰
    
    Args:
        rel_path: resourceãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰ã®ç›¸å¯¾ãƒ‘ã‚¹ï¼ˆæ‹¡å¼µå­ãªã—ï¼‰
        resource_dir: resourceãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹
        client: AzureOpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
        use_chunking: ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ã‚’ä½¿ç”¨ã™ã‚‹ã‹
        include_images: ç”»åƒã‚’å«ã‚ã‚‹ã‹
    
    Returns:
        å…¨ã‚·ãƒ¼ãƒˆã‹ã‚‰æŠ½å‡ºã•ã‚ŒãŸæƒ…å ±ã‚’çµ±åˆã—ãŸMarkdownæ–‡å­—åˆ—
    """
    basename = os.path.basename(rel_path)
    
    excel_path = os.path.join(resource_dir, f'{rel_path}.xlsx')
    if not os.path.exists(excel_path):
        excel_path = os.path.join(resource_dir, f'{rel_path}.xls')
    
    all_sheets = pd.read_excel(excel_path, sheet_name=None)
    
    extracted_parts = [f"# {basename} - æŠ½å‡ºæƒ…å ±\n\n"]
    
    for i, (sheet_name, df) in enumerate(all_sheets.items(), 1):
        print(f'  Processing sheet {i}/{len(all_sheets)}: {sheet_name} ({len(df)} rows)')
        
        try:
            sheet_info = extract_info_from_sheet(
                basename, sheet_name, df, excel_path, client, use_chunking, include_images
            )
            
            extracted_parts.append(f"## ã‚·ãƒ¼ãƒˆ: {sheet_name}\n\n")
            extracted_parts.append(sheet_info)
            extracted_parts.append("\n\n---\n\n")
            
            if i < len(all_sheets):
                time.sleep(WAIT_TIME_BETWEEN_SHEETS)
            
        except Exception as e:
            print(f'  âš  Error processing sheet {sheet_name}: {str(e)}')
            extracted_parts.append(f"## ã‚·ãƒ¼ãƒˆ: {sheet_name}\n\n")
            extracted_parts.append(f"**ã‚¨ãƒ©ãƒ¼**: {str(e)}\n\n---\n\n")
    
    return "".join(extracted_parts)


def main(use_chunking: bool = False, include_images: bool = True):
    """
    ãƒ¡ã‚¤ãƒ³å‡¦ç†
    
    Args:
        use_chunking: å¤§ããªã‚·ãƒ¼ãƒˆã‚’ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ã™ã‚‹ã‹
        include_images: ç”»åƒã‚’å«ã‚ã‚‹ã‹
    """
    inputs = get_excel_files(INPUT_DIR)
    
    # å‡ºåŠ›æ¸ˆã¿ã®ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—
    output_mds = []
    if os.path.exists(OUTPUT_DIR):
        for root, dirs, files in os.walk(OUTPUT_DIR):
            for file in files:
                if file.endswith('.md'):
                    full_path = os.path.join(root, file)
                    rel_path = os.path.relpath(full_path, OUTPUT_DIR)
                    rel_path_without_ext = os.path.splitext(rel_path)[0]
                    output_mds.append(rel_path_without_ext)
    
    # æœªå‡¦ç†ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æŠ½å‡º
    undescribeds = [f for f in inputs if f not in output_mds]
    
    print(f'Found {len(inputs)} Excel files')
    print(f'Already processed: {len(output_mds)}')
    print(f'Processing {len(undescribeds)} files...')
    print(f'Chunking mode: {"ON" if use_chunking else "OFF"}')
    print(f'Include images: {"ON" if include_images else "OFF"}')
    
    if len(undescribeds) == 0:
        print('No files to process.')
        return
    
    client = get_client()
    
    for idx, undescribed in enumerate(undescribeds, 1):
        try:
            print(f'\n[{idx}/{len(undescribeds)}] ğŸ“„ Processing: {undescribed}')
            description = extract_info(undescribed, INPUT_DIR, client, use_chunking, include_images)
            
            output_path = os.path.join(OUTPUT_DIR, f'{undescribed}.md')
            output_dir = os.path.dirname(output_path)
            if output_dir and not os.path.exists(output_dir):
                os.makedirs(output_dir, exist_ok=True)
            
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(description)
            
            print(f'âœ“ Information file created: {undescribed}')
            
            if idx < len(undescribeds):
                time.sleep(WAIT_TIME_BETWEEN_FILES)
            
        except Exception as e:
            print(f'âœ— Error processing {undescribed}: {str(e)}')
    
    print(f'\n{"="*60}')
    print(f'Processing complete!')


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='Extract information from Excel files with image support')
    parser.add_argument('--chunking', action='store_true', 
                       help='Enable chunking for large sheets')
    parser.add_argument('--no-images', action='store_true',
                       help='Disable image extraction')
    
    args = parser.parse_args()
    
    main(use_chunking=args.chunking, include_images=not args.no_images)
