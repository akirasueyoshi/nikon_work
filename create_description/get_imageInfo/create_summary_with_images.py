import pandas as pd
import os
import time
import openpyxl
from PIL import Image
import io
import base64
from typing import Dict, List
from azure.identity import InteractiveBrowserCredential, get_bearer_token_provider
from openai import AzureOpenAI

AZURE_OPENAI_ENDPOINT = 'https://aoai-je-exm.openai.azure.com/'
DEPLOYMENT_NAME = 'gpt-4o'  # gpt-4oã¯visionå¯¾å¿œ

# æ®µéš1: å„ã‚·ãƒ¼ãƒˆã®æ¦‚è¦ã‚’ä½œæˆ
SHEET_SUMMARY_PROMPT = """
ä»¥ä¸‹ã¯åŠå°ä½“éœ²å…‰è£…ç½®ã«ãŠã‘ã‚‹{basename}ã¨ã„ã†æ©Ÿèƒ½ä»•æ§˜æ›¸ã®ã€Œ{sheet_name}ã€ã‚·ãƒ¼ãƒˆã®å†…å®¹ã§ã™ã€‚
ã“ã®ã‚·ãƒ¼ãƒˆã®å†…å®¹ã‚’ç°¡æ½”ã«è¦ç´„ã—ã¦ãã ã•ã„ï¼ˆ200-300æ–‡å­—ç¨‹åº¦ï¼‰ã€‚

è¦ç´„ã«ã¯ä»¥ä¸‹ã‚’å«ã‚ã¦ãã ã•ã„ï¼š
- ã“ã®ã‚·ãƒ¼ãƒˆã®ç›®çš„ãƒ»å½¹å‰²
- ä¸»è¦ãªæƒ…å ±ã®ç¨®é¡ï¼ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€ãƒ•ãƒ­ãƒ¼ã€çŠ¶æ…‹é·ç§»ãªã©ï¼‰
- é‡è¦ãªãƒã‚¤ãƒ³ãƒˆ
- ç”»åƒãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã¯ã€ãã®å†…å®¹ã‚‚è¦ç´„ã«å«ã‚ã¦ãã ã•ã„
"""

# æ®µéš2: å…¨ä½“ã®è§£èª¬ã‚’ä½œæˆ
FINAL_SUMMARY_PROMPT = """
ä»¥ä¸‹ã¯åŠå°ä½“éœ²å…‰è£…ç½®ã«ãŠã‘ã‚‹{basename}ã¨ã„ã†æ©Ÿèƒ½ä»•æ§˜æ›¸ã®å…¨ã‚·ãƒ¼ãƒˆæ¦‚è¦ã§ã™ã€‚
ã“ã‚Œã‚‰ã®æƒ…å ±ã‚’çµ±åˆã—ã¦ã€ã“ã®æ©Ÿèƒ½ã®åŒ…æ‹¬çš„ãªè§£èª¬ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

## è§£èª¬ã«å«ã‚ã‚‹ã¹ãå†…å®¹
1. **æ©Ÿèƒ½æ¦‚è¦**: ã“ã®æ©Ÿèƒ½ã®ç›®çš„ã¨å½¹å‰²
2. **ä¸»è¦ãªå‡¦ç†ãƒ•ãƒ­ãƒ¼**: ã©ã®ã‚ˆã†ãªå‡¦ç†ãŒè¡Œã‚ã‚Œã‚‹ã‹
3. **å…¥å‡ºåŠ›**: ä½•ã‚’å—ã‘å–ã‚Šã€ä½•ã‚’å‡ºåŠ›ã™ã‚‹ã‹
4. **é‡è¦ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿**: ã‚­ãƒ¼ã¨ãªã‚‹è¨­å®šå€¤ã‚„åˆ¶ç´„
5. **é–¢é€£ã™ã‚‹æ©Ÿèƒ½**: ä»–ã®æ©Ÿèƒ½ã¨ã®é–¢ä¿‚æ€§
6. **ç‰¹è¨˜äº‹é …**: æ³¨æ„ã™ã¹ãç‚¹ã‚„åˆ¶ç´„æ¡ä»¶

å›ç­”ã¯Markdownå½¢å¼ã§æ§‹é€ åŒ–ã—ã¦ãã ã•ã„ã€‚
"""

INPUT_DIR = r'resource'
OUTPUT_DIR = r'summary'

# ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–ã®å¾…æ©Ÿæ™‚é–“ï¼ˆç§’ï¼‰
WAIT_TIME_BETWEEN_SHEETS = 2
WAIT_TIME_BETWEEN_FILES = 5

# ã‚·ãƒ¼ãƒˆã‚’åˆ†å‰²ã™ã‚‹é–¾å€¤ï¼ˆè¡Œæ•°ï¼‰
MAX_ROWS_PER_CHUNK = 100


def get_excel_files(dir_path: str) -> list[str]:
    """
    æŒ‡å®šãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªé…ä¸‹ã®ã™ã¹ã¦ã®Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’å†å¸°çš„ã«å–å¾—
    ãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰ã®ç›¸å¯¾ãƒ‘ã‚¹ï¼ˆæ‹¡å¼µå­ãªã—ï¼‰ã®ãƒªã‚¹ãƒˆã‚’è¿”ã™
    """
    excel_files = []
    
    if not os.path.exists(dir_path):
        os.makedirs(dir_path)
        return excel_files
    
    for root, dirs, files in os.walk(dir_path):
        for file in files:
            if file.endswith(('.xlsx', '.xls')):
                full_path = os.path.join(root, file)
                rel_path = os.path.relpath(full_path, dir_path)
                rel_path_without_ext = os.path.splitext(rel_path)[0]
                excel_files.append(rel_path_without_ext)
    
    return excel_files


def get_client() -> AzureOpenAI:
    credential = InteractiveBrowserCredential(
        tenant_id="4876a51c-4f2d-4d54-b712-e0b67d308e80"
    )
    
    token_provider = get_bearer_token_provider(
        credential,
        "https://cognitiveservices.azure.com/.default"
    )

    return AzureOpenAI(
        azure_endpoint=AZURE_OPENAI_ENDPOINT,
        azure_ad_token_provider=token_provider,
        api_version="2024-12-01-preview",
    )


def extract_images_from_sheet(excel_path: str, sheet_name: str) -> List[Image.Image]:
    """
    ç‰¹å®šã®ã‚·ãƒ¼ãƒˆã‹ã‚‰ç”»åƒã‚’æŠ½å‡º
    
    Args:
        excel_path: Excelãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        sheet_name: ã‚·ãƒ¼ãƒˆå
    
    Returns:
        PIL Imageã®ãƒªã‚¹ãƒˆ
    """
    try:
        wb = openpyxl.load_workbook(excel_path)
        sheet = wb[sheet_name]
        
        images = []
        if hasattr(sheet, '_images'):
            for img in sheet._images:
                image_data = img._data()
                pil_image = Image.open(io.BytesIO(image_data))
                images.append(pil_image)
        
        return images
    except Exception as e:
        print(f'    Warning: Could not extract images from {sheet_name}: {str(e)}')
        return []


def image_to_base64(image: Image.Image) -> str:
    """
    PIL Imageã‚’Base64æ–‡å­—åˆ—ã«å¤‰æ›
    """
    buffered = io.BytesIO()
    # å¤§ããªç”»åƒã¯ãƒªã‚µã‚¤ã‚º
    max_size = (1024, 1024)
    image.thumbnail(max_size, Image.Resampling.LANCZOS)
    image.save(buffered, format="PNG")
    img_str = base64.b64encode(buffered.getvalue()).decode()
    return img_str


def create_message_content(text: str, images: List[Image.Image]) -> list:
    """
    ãƒ†ã‚­ã‚¹ãƒˆã¨ç”»åƒã‚’å«ã‚€ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ä½œæˆ
    """
    content = [{"type": "text", "text": text}]
    
    # ç”»åƒã‚’è¿½åŠ ï¼ˆæœ€å¤§5æšã¾ã§ï¼‰
    for img in images[:5]:
        base64_image = image_to_base64(img)
        content.append({
            "type": "image_url",
            "image_url": {
                "url": f"data:image/png;base64,{base64_image}"
            }
        })
    
    return content


def split_dataframe_into_chunks(df: pd.DataFrame, max_rows: int = MAX_ROWS_PER_CHUNK) -> List[tuple]:
    """DataFrameã‚’æŒ‡å®šè¡Œæ•°ã§åˆ†å‰²"""
    chunks = []
    total_rows = len(df)
    
    for start in range(0, total_rows, max_rows):
        end = min(start + max_rows, total_rows)
        chunk = df.iloc[start:end]
        chunks.append((start, end, chunk))
    
    return chunks


def summarize_sheet(basename: str, sheet_name: str, df: pd.DataFrame, 
                   excel_path: str, client: AzureOpenAI,
                   use_chunking: bool = False, include_images: bool = True) -> str:
    """
    1ã¤ã®ã‚·ãƒ¼ãƒˆã®æ¦‚è¦ã‚’ä½œæˆï¼ˆç”»åƒå¯¾å¿œç‰ˆï¼‰
    """
    # ç”»åƒã‚’æŠ½å‡º
    images = []
    if include_images:
        images = extract_images_from_sheet(excel_path, sheet_name)
        if images:
            print(f'    Found {len(images)} images in sheet')
    
    # ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ãŒä¸è¦ãªå ´åˆ
    if not use_chunking or len(df) <= MAX_ROWS_PER_CHUNK:
        table_text = df.to_markdown()
        user_content = create_message_content(table_text, images)
        
        response = client.chat.completions.create(
            model=DEPLOYMENT_NAME,
            messages=[
                {"role": "system", "content": SHEET_SUMMARY_PROMPT.format(
                    basename=basename, 
                    sheet_name=sheet_name
                )},
                {"role": "user", "content": user_content}
            ],
            max_tokens=1000
        )
        return response.choices[0].message.content
    
    # ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ã—ã¦å‡¦ç†
    chunks = split_dataframe_into_chunks(df)
    chunk_summaries = []
    
    print(f'    Splitting into {len(chunks)} chunks...')
    
    for i, (start, end, chunk_df) in enumerate(chunks, 1):
        chunk_info = f"è¡Œ{start+1}-{end}"
        print(f'    Processing chunk {i}/{len(chunks)}: {chunk_info}')
        
        try:
            table_text = chunk_df.to_markdown()
            
            # æœ€åˆã®ãƒãƒ£ãƒ³ã‚¯ã«ã®ã¿ç”»åƒã‚’å«ã‚ã‚‹
            chunk_images = images if i == 1 else []
            user_content = create_message_content(table_text, chunk_images)
            
            response = client.chat.completions.create(
                model=DEPLOYMENT_NAME,
                messages=[
                    {"role": "system", "content": SHEET_SUMMARY_PROMPT.format(
                        basename=basename, 
                        sheet_name=f"{sheet_name} ({chunk_info})"
                    )},
                    {"role": "user", "content": user_content}
                ],
                max_tokens=1000
            )
            chunk_summaries.append(response.choices[0].message.content)
            
            if i < len(chunks):
                time.sleep(WAIT_TIME_BETWEEN_SHEETS)
                
        except Exception as e:
            print(f'    âš  Error processing chunk {chunk_info}: {str(e)}')
            chunk_summaries.append(f"[{chunk_info}] ã‚¨ãƒ©ãƒ¼: {str(e)}")
    
    return "\n\n".join(chunk_summaries)


def create_sheet_summaries(basename: str, all_sheets: Dict[str, pd.DataFrame], 
                          excel_path: str, client: AzureOpenAI, 
                          use_chunking: bool = False, include_images: bool = True) -> Dict[str, str]:
    """
    å…¨ã‚·ãƒ¼ãƒˆã®æ¦‚è¦ã‚’ä½œæˆ
    """
    sheet_summaries = {}
    
    for i, (sheet_name, df) in enumerate(all_sheets.items(), 1):
        print(f'  Summarizing sheet {i}/{len(all_sheets)}: {sheet_name} ({len(df)} rows)')
        
        try:
            summary = summarize_sheet(basename, sheet_name, df, excel_path, client, 
                                     use_chunking, include_images)
            sheet_summaries[sheet_name] = summary
            
            if i < len(all_sheets):
                time.sleep(WAIT_TIME_BETWEEN_SHEETS)
                
        except Exception as e:
            print(f'  âš  Error summarizing sheet {sheet_name}: {str(e)}')
            sheet_summaries[sheet_name] = f"ã‚¨ãƒ©ãƒ¼: {str(e)}"
    
    return sheet_summaries


def create_final_summary(basename: str, sheet_summaries: Dict[str, str], 
                        client: AzureOpenAI) -> str:
    """
    å…¨ã‚·ãƒ¼ãƒˆæ¦‚è¦ã‹ã‚‰æœ€çµ‚çš„ãªè§£èª¬ã‚’ä½œæˆ
    """
    summaries_text = []
    for sheet_name, summary in sheet_summaries.items():
        summaries_text.append(f"### {sheet_name}\n{summary}\n")
    
    combined_summaries = "\n".join(summaries_text)
    
    response = client.chat.completions.create(
        model=DEPLOYMENT_NAME,
        messages=[
            {"role": "system", "content": FINAL_SUMMARY_PROMPT.format(basename=basename)},
            {"role": "user", "content": combined_summaries}
        ],
        max_tokens=10000
    )
    return response.choices[0].message.content


def create_summary(rel_path: str, resource_dir: str, client: AzureOpenAI, 
                  use_chunking: bool = False, include_images: bool = True) -> str:
    """
    Excelãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰è§£èª¬ã‚’ä½œæˆï¼ˆ2æ®µéšæ–¹å¼ã€ç”»åƒå¯¾å¿œï¼‰
    """
    basename = os.path.basename(rel_path)
    
    excel_path = os.path.join(resource_dir, f'{rel_path}.xlsx')
    if not os.path.exists(excel_path):
        excel_path = os.path.join(resource_dir, f'{rel_path}.xls')
    
    all_sheets = pd.read_excel(excel_path, sheet_name=None)
    print(f'  Found {len(all_sheets)} sheets')
    
    # æ®µéš1: å„ã‚·ãƒ¼ãƒˆã®æ¦‚è¦ã‚’ä½œæˆ
    print(f'  Phase 1: Creating sheet summaries...')
    sheet_summaries = create_sheet_summaries(basename, all_sheets, excel_path, client, 
                                            use_chunking, include_images)
    
    # æ®µéš2: å…¨ä½“ã®è§£èª¬ã‚’ä½œæˆ
    print(f'  Phase 2: Creating final summary...')
    time.sleep(WAIT_TIME_BETWEEN_SHEETS)
    final_summary = create_final_summary(basename, sheet_summaries, client)
    
    # æœ€çµ‚çš„ãªMarkdownã‚’æ§‹ç¯‰
    result_parts = [
        f"# {basename} - æ©Ÿèƒ½è§£èª¬\n\n",
        final_summary,
        "\n\n---\n\n",
        "## å„ã‚·ãƒ¼ãƒˆæ¦‚è¦\n\n"
    ]
    
    for sheet_name, summary in sheet_summaries.items():
        result_parts.append(f"### {sheet_name}\n\n{summary}\n\n")
    
    return "".join(result_parts)


def main(use_chunking: bool = False, include_images: bool = True):
    """
    ãƒ¡ã‚¤ãƒ³å‡¦ç†
    """
    inputs = get_excel_files(INPUT_DIR)
    
    output_mds = []
    if os.path.exists(OUTPUT_DIR):
        for root, dirs, files in os.walk(OUTPUT_DIR):
            for file in files:
                if file.endswith('.md'):
                    full_path = os.path.join(root, file)
                    rel_path = os.path.relpath(full_path, OUTPUT_DIR)
                    rel_path_without_ext = os.path.splitext(rel_path)[0]
                    output_mds.append(rel_path_without_ext)
    
    undescribeds = [f for f in inputs if f not in output_mds]
    
    print(f'Found {len(inputs)} Excel files')
    print(f'Already processed: {len(output_mds)}')
    print(f'Processing {len(undescribeds)} files...')
    print(f'Chunking mode: {"ON" if use_chunking else "OFF"}')
    print(f'Include images: {"ON" if include_images else "OFF"}')
    
    if len(undescribeds) == 0:
        print('No files to process.')
        return
    
    client = get_client()
    
    for idx, undescribed in enumerate(undescribeds, 1):
        try:
            print(f'\n[{idx}/{len(undescribeds)}] ğŸ“„ Processing: {undescribed}')
            description = create_summary(undescribed, INPUT_DIR, client, use_chunking, include_images)
            
            output_path = os.path.join(OUTPUT_DIR, f'{undescribed}.md')
            output_dir = os.path.dirname(output_path)
            if output_dir and not os.path.exists(output_dir):
                os.makedirs(output_dir, exist_ok=True)
            
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(description)
            
            print(f'âœ“ Summary file created: {undescribed}')
            
            if idx < len(undescribeds):
                time.sleep(WAIT_TIME_BETWEEN_FILES)
            
        except Exception as e:
            print(f'âœ— Error processing {undescribed}: {str(e)}')
    
    print(f'\n{"="*60}')
    print(f'Processing complete!')


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='Create summary from Excel files with image support')
    parser.add_argument('--chunking', action='store_true', 
                       help='Enable chunking for large sheets')
    parser.add_argument('--no-images', action='store_true',
                       help='Disable image extraction')
    
    args = parser.parse_args()
    
    main(use_chunking=args.chunking, include_images=not args.no_images)
