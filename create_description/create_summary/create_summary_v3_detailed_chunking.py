import pandas as pd
import os
import time
from typing import Dict, List
from azure.identity import InteractiveBrowserCredential, get_bearer_token_provider
from openai import AzureOpenAI

AZURE_OPENAI_ENDPOINT = 'https://aoai-je-exm.openai.azure.com/'
DEPLOYMENT_NAME = 'gpt-4o'

# å„ã‚·ãƒ¼ãƒˆã®è©³ç´°è§£èª¬ã‚’ä½œæˆ
SHEET_DETAIL_PROMPT = """
ä»¥ä¸‹ã¯åŠå°ä½“éœ²å…‰è£…ç½®ã«ãŠã‘ã‚‹{basename}ã¨ã„ã†æ©Ÿèƒ½ä»•æ§˜æ›¸ã®ã€Œ{sheet_name}ã€ã‚·ãƒ¼ãƒˆã®å†…å®¹ã§ã™ã€‚
ã“ã®ã‚·ãƒ¼ãƒˆã®å†…å®¹ã«ã¤ã„ã¦è©³ç´°ãªè§£èª¬ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

## è§£èª¬ã«å«ã‚ã‚‹ã¹ãå†…å®¹
- ã‚·ãƒ¼ãƒˆã®ç›®çš„ã¨å½¹å‰²
- ä¸»è¦ãªæƒ…å ±ã®èª¬æ˜
- ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚„è¨­å®šå€¤ã®æ„å‘³
- å‡¦ç†ãƒ•ãƒ­ãƒ¼ã‚„çŠ¶æ…‹é·ç§»ï¼ˆè©²å½“ã™ã‚‹å ´åˆï¼‰
- æ³¨æ„ã™ã¹ãç‚¹ã‚„åˆ¶ç´„

å›ç­”ã¯Markdownå½¢å¼ã§ã€è¦‹å‡ºã—ã‚’ä½¿ã£ã¦æ§‹é€ åŒ–ã—ã¦ãã ã•ã„ã€‚
"""

# çµ±åˆè§£èª¬ã‚’ä½œæˆ
INTEGRATION_PROMPT = """
ä»¥ä¸‹ã¯åŠå°ä½“éœ²å…‰è£…ç½®ã«ãŠã‘ã‚‹{basename}ã¨ã„ã†æ©Ÿèƒ½ä»•æ§˜æ›¸ã®å„ã‚·ãƒ¼ãƒˆè§£èª¬ã§ã™ã€‚
ã“ã‚Œã‚‰ã®æƒ…å ±ã‚’çµ±åˆã—ã¦ã€æ©Ÿèƒ½å…¨ä½“ã®åŒ…æ‹¬çš„ãªè§£èª¬ã‚’å†’é ­ã«è¿½åŠ ã—ã¦ãã ã•ã„ã€‚

## çµ±åˆè§£èª¬ã«å«ã‚ã‚‹ã¹ãå†…å®¹
1. **æ©Ÿèƒ½å…¨ä½“ã®æ¦‚è¦**: ã“ã®æ©Ÿèƒ½ã®ç›®çš„ã¨ä½ç½®ã¥ã‘
2. **ä¸»è¦ãªå‡¦ç†ãƒ•ãƒ­ãƒ¼**: å…¨ä½“çš„ãªå‡¦ç†ã®æµã‚Œ
3. **ã‚·ãƒ¼ãƒˆé–“ã®é–¢ä¿‚**: å„ã‚·ãƒ¼ãƒˆãŒã©ã†é–¢é€£ã—ã¦ã„ã‚‹ã‹
4. **é‡è¦ãªãƒã‚¤ãƒ³ãƒˆ**: ç†è§£ã—ã¦ãŠãã¹ãé‡è¦äº‹é …
5. **ä½¿ç”¨ã‚·ãƒ¼ãƒ³**: ã©ã®ã‚ˆã†ãªå ´é¢ã§ä½¿ç”¨ã•ã‚Œã‚‹ã‹

çµ±åˆè§£èª¬ã‚’ä½œæˆã—ãŸã‚‰ã€ãã®å¾Œã«å„ã‚·ãƒ¼ãƒˆã®è©³ç´°è§£èª¬ã‚’ç¶šã‘ã¦ãã ã•ã„ã€‚
å…¨ä½“ã‚’Markdownå½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
"""

INPUT_DIR = r'resource'
OUTPUT_DIR = r'summary'

# ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–ã®å¾…æ©Ÿæ™‚é–“ï¼ˆç§’ï¼‰
WAIT_TIME_BETWEEN_SHEETS = 2
WAIT_TIME_BETWEEN_FILES = 5

# ã‚·ãƒ¼ãƒˆã‚’åˆ†å‰²ã™ã‚‹é–¾å€¤ï¼ˆè¡Œæ•°ï¼‰
MAX_ROWS_PER_CHUNK = 100


def get_excel_files(dir_path: str) -> list[str]:
    """
    æŒ‡å®šãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªé…ä¸‹ã®ã™ã¹ã¦ã®Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’å†å¸°çš„ã«å–å¾—
    ãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰ã®ç›¸å¯¾ãƒ‘ã‚¹ï¼ˆæ‹¡å¼µå­ãªã—ï¼‰ã®ãƒªã‚¹ãƒˆã‚’è¿”ã™
    """
    excel_files = []
    
    if not os.path.exists(dir_path):
        os.makedirs(dir_path)
        return excel_files
    
    for root, dirs, files in os.walk(dir_path):
        for file in files:
            if file.endswith(('.xlsx', '.xls')):
                full_path = os.path.join(root, file)
                rel_path = os.path.relpath(full_path, dir_path)
                rel_path_without_ext = os.path.splitext(rel_path)[0]
                excel_files.append(rel_path_without_ext)
    
    return excel_files


def get_client() -> AzureOpenAI:
    credential = InteractiveBrowserCredential(
        tenant_id="4876a51c-4f2d-4d54-b712-e0b67d308e80"
    )
    
    token_provider = get_bearer_token_provider(
        credential,
        "https://cognitiveservices.azure.com/.default"
    )

    return AzureOpenAI(
        azure_endpoint=AZURE_OPENAI_ENDPOINT,
        azure_ad_token_provider=token_provider,
        api_version="2024-12-01-preview",
    )


def split_dataframe_into_chunks(df: pd.DataFrame, max_rows: int = MAX_ROWS_PER_CHUNK) -> List[tuple]:
    """
    DataFrameã‚’æŒ‡å®šè¡Œæ•°ã§åˆ†å‰²
    
    Args:
        df: åˆ†å‰²ã™ã‚‹DataFrame
        max_rows: ãƒãƒ£ãƒ³ã‚¯ã‚ãŸã‚Šã®æœ€å¤§è¡Œæ•°
    
    Returns:
        (é–‹å§‹è¡Œ, çµ‚äº†è¡Œ, ãƒãƒ£ãƒ³ã‚¯)ã®ãƒªã‚¹ãƒˆ
    """
    chunks = []
    total_rows = len(df)
    
    for start in range(0, total_rows, max_rows):
        end = min(start + max_rows, total_rows)
        chunk = df.iloc[start:end]
        chunks.append((start, end, chunk))
    
    return chunks


def create_sheet_detail(basename: str, sheet_name: str, df: pd.DataFrame, 
                       client: AzureOpenAI, use_chunking: bool = False) -> str:
    """
    1ã¤ã®ã‚·ãƒ¼ãƒˆã®è©³ç´°è§£èª¬ã‚’ä½œæˆï¼ˆå¿…è¦ã«å¿œã˜ã¦ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ï¼‰
    
    Args:
        basename: ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ™ãƒ¼ã‚¹å
        sheet_name: ã‚·ãƒ¼ãƒˆå
        df: ã‚·ãƒ¼ãƒˆã®DataFrame
        client: AzureOpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
        use_chunking: ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ã‚’ä½¿ç”¨ã™ã‚‹ã‹
    
    Returns:
        ã‚·ãƒ¼ãƒˆã®è©³ç´°è§£èª¬ï¼ˆMarkdownå½¢å¼ï¼‰
    """
    # ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ãŒä¸è¦ãªå ´åˆ
    if not use_chunking or len(df) <= MAX_ROWS_PER_CHUNK:
        table_text = df.to_markdown()
        
        response = client.chat.completions.create(
            model=DEPLOYMENT_NAME,
            messages=[
                {"role": "system", "content": SHEET_DETAIL_PROMPT.format(
                    basename=basename, 
                    sheet_name=sheet_name
                )},
                {"role": "user", "content": table_text}
            ],
            max_tokens=5000
        )
        return response.choices[0].message.content
    
    # ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ã—ã¦å‡¦ç†
    chunks = split_dataframe_into_chunks(df)
    chunk_details = []
    
    print(f'    Splitting into {len(chunks)} chunks...')
    
    for i, (start, end, chunk_df) in enumerate(chunks, 1):
        chunk_info = f"è¡Œ{start+1}-{end}"
        print(f'    Processing chunk {i}/{len(chunks)}: {chunk_info}')
        
        try:
            table_text = chunk_df.to_markdown()
            
            response = client.chat.completions.create(
                model=DEPLOYMENT_NAME,
                messages=[
                    {"role": "system", "content": SHEET_DETAIL_PROMPT.format(
                        basename=basename, 
                        sheet_name=f"{sheet_name} ({chunk_info})"
                    )},
                    {"role": "user", "content": table_text}
                ],
                max_tokens=5000
            )
            chunk_details.append(f"### {chunk_info}\n\n{response.choices[0].message.content}")
            
            if i < len(chunks):
                time.sleep(WAIT_TIME_BETWEEN_SHEETS)
                
        except Exception as e:
            print(f'    âš  Error processing chunk {chunk_info}: {str(e)}')
            chunk_details.append(f"### {chunk_info}\n\n**ã‚¨ãƒ©ãƒ¼**: {str(e)}")
    
    # ãƒãƒ£ãƒ³ã‚¯è©³ç´°ã‚’çµ±åˆ
    return "\n\n".join(chunk_details)


def create_sheet_details(basename: str, all_sheets: Dict[str, pd.DataFrame], 
                        client: AzureOpenAI, use_chunking: bool = False) -> Dict[str, str]:
    """
    å…¨ã‚·ãƒ¼ãƒˆã®è©³ç´°è§£èª¬ã‚’ä½œæˆ
    
    Args:
        basename: ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ™ãƒ¼ã‚¹å
        all_sheets: {ã‚·ãƒ¼ãƒˆå: DataFrame}ã®è¾æ›¸
        client: AzureOpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
        use_chunking: ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ã‚’ä½¿ç”¨ã™ã‚‹ã‹
    
    Returns:
        {ã‚·ãƒ¼ãƒˆå: è©³ç´°è§£èª¬}ã®è¾æ›¸
    """
    sheet_details = {}
    
    for i, (sheet_name, df) in enumerate(all_sheets.items(), 1):
        print(f'  Creating detailed explanation for sheet {i}/{len(all_sheets)}: {sheet_name} ({len(df)} rows)')
        
        try:
            detail = create_sheet_detail(basename, sheet_name, df, client, use_chunking)
            sheet_details[sheet_name] = detail
            
            # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–
            if i < len(all_sheets):
                time.sleep(WAIT_TIME_BETWEEN_SHEETS)
                
        except Exception as e:
            print(f'  âš  Error processing sheet {sheet_name}: {str(e)}')
            sheet_details[sheet_name] = f"**ã‚¨ãƒ©ãƒ¼**: {str(e)}"
    
    return sheet_details


def integrate_explanations(basename: str, sheet_details: Dict[str, str], 
                          client: AzureOpenAI) -> str:
    """
    å„ã‚·ãƒ¼ãƒˆè§£èª¬ã‚’çµ±åˆã—ã¦å…¨ä½“è§£èª¬ã‚’è¿½åŠ 
    
    Args:
        basename: ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ™ãƒ¼ã‚¹å
        sheet_details: {ã‚·ãƒ¼ãƒˆå: è©³ç´°è§£èª¬}ã®è¾æ›¸
        client: AzureOpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
    
    Returns:
        çµ±åˆã•ã‚ŒãŸè§£èª¬ï¼ˆMarkdownå½¢å¼ï¼‰
    """
    # å„ã‚·ãƒ¼ãƒˆè§£èª¬ã‚’ãƒ†ã‚­ã‚¹ãƒˆã«æ•´å½¢
    details_text = []
    for sheet_name, detail in sheet_details.items():
        details_text.append(f"## {sheet_name}\n\n{detail}\n\n---\n")
    
    combined_details = "\n".join(details_text)
    
    response = client.chat.completions.create(
        model=DEPLOYMENT_NAME,
        messages=[
            {"role": "system", "content": INTEGRATION_PROMPT.format(basename=basename)},
            {"role": "user", "content": combined_details}
        ],
        max_tokens=16000
    )
    return response.choices[0].message.content


def create_summary_detailed(rel_path: str, resource_dir: str, client: AzureOpenAI,
                           use_chunking: bool = False) -> str:
    """
    è©³ç´°è§£èª¬æ–¹å¼ã§Excelãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰è§£èª¬ã‚’ä½œæˆ
    
    æ®µéš1: å„ã‚·ãƒ¼ãƒˆã®è©³ç´°è§£èª¬ã‚’ä½œæˆ
    æ®µéš2: è©³ç´°è§£èª¬ã‚’çµ±åˆã—ã¦å…¨ä½“è§£èª¬ã‚’è¿½åŠ 
    
    Args:
        rel_path: resourceãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰ã®ç›¸å¯¾ãƒ‘ã‚¹ï¼ˆæ‹¡å¼µå­ãªã—ï¼‰
        resource_dir: resourceãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹
        client: AzureOpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
        use_chunking: ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ã‚’ä½¿ç”¨ã™ã‚‹ã‹
    
    Returns:
        æœ€çµ‚çš„ãªè§£èª¬ï¼ˆMarkdownå½¢å¼ï¼‰
    """
    basename = os.path.basename(rel_path)
    
    # Excelãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ•ãƒ«ãƒ‘ã‚¹ã‚’æ§‹ç¯‰
    excel_path = os.path.join(resource_dir, f'{rel_path}.xlsx')
    if not os.path.exists(excel_path):
        excel_path = os.path.join(resource_dir, f'{rel_path}.xls')
    
    # å…¨ã‚·ãƒ¼ãƒˆã‚’èª­ã¿è¾¼ã‚€
    all_sheets = pd.read_excel(excel_path, sheet_name=None)
    print(f'  Found {len(all_sheets)} sheets')
    
    # æ®µéš1: å„ã‚·ãƒ¼ãƒˆã®è©³ç´°è§£èª¬ã‚’ä½œæˆ
    print(f'  Phase 1: Creating detailed explanations for each sheet...')
    sheet_details = create_sheet_details(basename, all_sheets, client, use_chunking)
    
    # æ®µéš2: çµ±åˆè§£èª¬ã‚’ä½œæˆ
    print(f'  Phase 2: Integrating explanations...')
    time.sleep(WAIT_TIME_BETWEEN_SHEETS)
    integrated_summary = integrate_explanations(basename, sheet_details, client)
    
    return integrated_summary


def main(use_chunking: bool = False):
    """
    ãƒ¡ã‚¤ãƒ³å‡¦ç†
    
    Args:
        use_chunking: å¤§ããªã‚·ãƒ¼ãƒˆã‚’ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ã™ã‚‹ã‹
    """
    inputs = get_excel_files(INPUT_DIR)
    
    # å‡ºåŠ›æ¸ˆã¿ã®ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—
    output_mds = []
    if os.path.exists(OUTPUT_DIR):
        for root, dirs, files in os.walk(OUTPUT_DIR):
            for file in files:
                if file.endswith('.md'):
                    full_path = os.path.join(root, file)
                    rel_path = os.path.relpath(full_path, OUTPUT_DIR)
                    rel_path_without_ext = os.path.splitext(rel_path)[0]
                    output_mds.append(rel_path_without_ext)
    
    # æœªå‡¦ç†ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æŠ½å‡º
    undescribeds = [f for f in inputs if f not in output_mds]
    
    print(f'Found {len(inputs)} Excel files')
    print(f'Already processed: {len(output_mds)}')
    print(f'Processing {len(undescribeds)} files...')
    print(f'Chunking mode: {"ON" if use_chunking else "OFF"}')
    
    if len(undescribeds) == 0:
        print('No files to process.')
        return
    
    client = get_client()
    
    for idx, undescribed in enumerate(undescribeds, 1):
        try:
            print(f'\n[{idx}/{len(undescribeds)}] ğŸ“„ Processing: {undescribed}')
            
            # è©³ç´°ãƒ¢ãƒ¼ãƒ‰ã§è§£èª¬ä½œæˆ
            description = create_summary_detailed(undescribed, INPUT_DIR, client, use_chunking)
            
            # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’æ§‹ç¯‰
            output_path = os.path.join(OUTPUT_DIR, f'{undescribed}.md')
            
            # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆï¼ˆå­˜åœ¨ã—ãªã„å ´åˆï¼‰
            output_dir = os.path.dirname(output_path)
            if output_dir and not os.path.exists(output_dir):
                os.makedirs(output_dir, exist_ok=True)
            
            # ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(description)
            
            print(f'âœ“ Summary file created: {undescribed}')
            
            # ãƒ•ã‚¡ã‚¤ãƒ«é–“ã®å¾…æ©Ÿ
            if idx < len(undescribeds):
                time.sleep(WAIT_TIME_BETWEEN_FILES)
            
        except Exception as e:
            print(f'âœ— Error processing {undescribed}: {str(e)}')
    
    print(f'\n{"="*60}')
    print(f'Processing complete!')


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='Create detailed summary from Excel files')
    parser.add_argument('--chunking', action='store_true', 
                       help='Enable chunking for large sheets')
    
    args = parser.parse_args()
    
    main(use_chunking=args.chunking)
