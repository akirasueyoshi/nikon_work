import pandas as pd
import os
import time
from typing import Dict, List
from azure.identity import InteractiveBrowserCredential, get_bearer_token_provider
from openai import AzureOpenAI

AZURE_OPENAI_ENDPOINT = 'https://aoai-je-exm.openai.azure.com/'
DEPLOYMENT_NAME = 'gpt-4o'

# æ®µéš1: å„ã‚·ãƒ¼ãƒˆã®æ¦‚è¦ã‚’ä½œæˆ
SHEET_SUMMARY_PROMPT = """
ä»¥ä¸‹ã¯åŠå°ä½“éœ²å…‰è£…ç½®ã«ãŠã‘ã‚‹{basename}ã¨ã„ã†æ©Ÿèƒ½ä»•æ§˜æ›¸ã®ã€Œ{sheet_name}ã€ã‚·ãƒ¼ãƒˆã®å†…å®¹ã§ã™ã€‚
ã“ã®ã‚·ãƒ¼ãƒˆã®å†…å®¹ã‚’ç°¡æ½”ã«è¦ç´„ã—ã¦ãã ã•ã„ï¼ˆ200-300æ–‡å­—ç¨‹åº¦ï¼‰ã€‚

è¦ç´„ã«ã¯ä»¥ä¸‹ã‚’å«ã‚ã¦ãã ã•ã„ï¼š
- ã“ã®ã‚·ãƒ¼ãƒˆã®ç›®çš„ãƒ»å½¹å‰²
- ä¸»è¦ãªæƒ…å ±ã®ç¨®é¡ï¼ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€ãƒ•ãƒ­ãƒ¼ã€çŠ¶æ…‹é·ç§»ãªã©ï¼‰
- é‡è¦ãªãƒã‚¤ãƒ³ãƒˆ
"""

# æ®µéš2: å…¨ä½“ã®è§£èª¬ã‚’ä½œæˆ
FINAL_SUMMARY_PROMPT = """
ä»¥ä¸‹ã¯åŠå°ä½“éœ²å…‰è£…ç½®ã«ãŠã‘ã‚‹{basename}ã¨ã„ã†æ©Ÿèƒ½ä»•æ§˜æ›¸ã®å…¨ã‚·ãƒ¼ãƒˆæ¦‚è¦ã§ã™ã€‚
ã“ã‚Œã‚‰ã®æƒ…å ±ã‚’çµ±åˆã—ã¦ã€ã“ã®æ©Ÿèƒ½ã®åŒ…æ‹¬çš„ãªè§£èª¬ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

## è§£èª¬ã«å«ã‚ã‚‹ã¹ãå†…å®¹
1. **æ©Ÿèƒ½æ¦‚è¦**: ã“ã®æ©Ÿèƒ½ã®ç›®çš„ã¨å½¹å‰²
2. **ä¸»è¦ãªå‡¦ç†ãƒ•ãƒ­ãƒ¼**: ã©ã®ã‚ˆã†ãªå‡¦ç†ãŒè¡Œã‚ã‚Œã‚‹ã‹
3. **å…¥å‡ºåŠ›**: ä½•ã‚’å—ã‘å–ã‚Šã€ä½•ã‚’å‡ºåŠ›ã™ã‚‹ã‹
4. **é‡è¦ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿**: ã‚­ãƒ¼ã¨ãªã‚‹è¨­å®šå€¤ã‚„åˆ¶ç´„
5. **é–¢é€£ã™ã‚‹æ©Ÿèƒ½**: ä»–ã®æ©Ÿèƒ½ã¨ã®é–¢ä¿‚æ€§
6. **ç‰¹è¨˜äº‹é …**: æ³¨æ„ã™ã¹ãç‚¹ã‚„åˆ¶ç´„æ¡ä»¶

å›ç­”ã¯Markdownå½¢å¼ã§æ§‹é€ åŒ–ã—ã¦ãã ã•ã„ã€‚
"""

INPUT_DIR = r'resource'
OUTPUT_DIR = r'summary'

# ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–ã®å¾…æ©Ÿæ™‚é–“ï¼ˆç§’ï¼‰
WAIT_TIME_BETWEEN_SHEETS = 2
WAIT_TIME_BETWEEN_FILES = 5

# ã‚·ãƒ¼ãƒˆã‚’åˆ†å‰²ã™ã‚‹é–¾å€¤ï¼ˆè¡Œæ•°ï¼‰
MAX_ROWS_PER_CHUNK = 100


def get_excel_files(dir_path: str) -> list[str]:
    """
    æŒ‡å®šãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªé…ä¸‹ã®ã™ã¹ã¦ã®Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’å†å¸°çš„ã«å–å¾—
    ãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰ã®ç›¸å¯¾ãƒ‘ã‚¹ï¼ˆæ‹¡å¼µå­ãªã—ï¼‰ã®ãƒªã‚¹ãƒˆã‚’è¿”ã™
    """
    excel_files = []
    
    if not os.path.exists(dir_path):
        os.makedirs(dir_path)
        return excel_files
    
    for root, dirs, files in os.walk(dir_path):
        for file in files:
            if file.endswith(('.xlsx', '.xls')):
                full_path = os.path.join(root, file)
                rel_path = os.path.relpath(full_path, dir_path)
                rel_path_without_ext = os.path.splitext(rel_path)[0]
                excel_files.append(rel_path_without_ext)
    
    return excel_files


def get_client() -> AzureOpenAI:
    credential = InteractiveBrowserCredential(
        tenant_id="4876a51c-4f2d-4d54-b712-e0b67d308e80"
    )
    
    token_provider = get_bearer_token_provider(
        credential,
        "https://cognitiveservices.azure.com/.default"
    )

    return AzureOpenAI(
        azure_endpoint=AZURE_OPENAI_ENDPOINT,
        azure_ad_token_provider=token_provider,
        api_version="2024-12-01-preview",
    )


def split_dataframe_into_chunks(df: pd.DataFrame, max_rows: int = MAX_ROWS_PER_CHUNK) -> List[tuple]:
    """
    DataFrameã‚’æŒ‡å®šè¡Œæ•°ã§åˆ†å‰²
    
    Args:
        df: åˆ†å‰²ã™ã‚‹DataFrame
        max_rows: ãƒãƒ£ãƒ³ã‚¯ã‚ãŸã‚Šã®æœ€å¤§è¡Œæ•°
    
    Returns:
        (é–‹å§‹è¡Œ, çµ‚äº†è¡Œ, ãƒãƒ£ãƒ³ã‚¯)ã®ãƒªã‚¹ãƒˆ
    """
    chunks = []
    total_rows = len(df)
    
    for start in range(0, total_rows, max_rows):
        end = min(start + max_rows, total_rows)
        chunk = df.iloc[start:end]
        chunks.append((start, end, chunk))
    
    return chunks


def summarize_sheet(basename: str, sheet_name: str, df: pd.DataFrame, client: AzureOpenAI,
                   use_chunking: bool = False) -> str:
    """
    1ã¤ã®ã‚·ãƒ¼ãƒˆã®æ¦‚è¦ã‚’ä½œæˆï¼ˆå¿…è¦ã«å¿œã˜ã¦ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ï¼‰
    
    Args:
        basename: ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ™ãƒ¼ã‚¹å
        sheet_name: ã‚·ãƒ¼ãƒˆå
        df: ã‚·ãƒ¼ãƒˆã®DataFrame
        client: AzureOpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
        use_chunking: ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ã‚’ä½¿ç”¨ã™ã‚‹ã‹
    
    Returns:
        ã‚·ãƒ¼ãƒˆã®æ¦‚è¦ï¼ˆãƒ†ã‚­ã‚¹ãƒˆï¼‰
    """
    # ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ãŒä¸è¦ãªå ´åˆ
    if not use_chunking or len(df) <= MAX_ROWS_PER_CHUNK:
        table_text = df.to_markdown()
        
        response = client.chat.completions.create(
            model=DEPLOYMENT_NAME,
            messages=[
                {"role": "system", "content": SHEET_SUMMARY_PROMPT.format(
                    basename=basename, 
                    sheet_name=sheet_name
                )},
                {"role": "user", "content": table_text}
            ],
            max_tokens=1000
        )
        return response.choices[0].message.content
    
    # ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ã—ã¦å‡¦ç†
    chunks = split_dataframe_into_chunks(df)
    chunk_summaries = []
    
    print(f'    Splitting into {len(chunks)} chunks...')
    
    for i, (start, end, chunk_df) in enumerate(chunks, 1):
        chunk_info = f"è¡Œ{start+1}-{end}"
        print(f'    Processing chunk {i}/{len(chunks)}: {chunk_info}')
        
        try:
            table_text = chunk_df.to_markdown()
            
            response = client.chat.completions.create(
                model=DEPLOYMENT_NAME,
                messages=[
                    {"role": "system", "content": SHEET_SUMMARY_PROMPT.format(
                        basename=basename, 
                        sheet_name=f"{sheet_name} ({chunk_info})"
                    )},
                    {"role": "user", "content": table_text}
                ],
                max_tokens=1000
            )
            chunk_summaries.append(response.choices[0].message.content)
            
            if i < len(chunks):
                time.sleep(WAIT_TIME_BETWEEN_SHEETS)
                
        except Exception as e:
            print(f'    âš  Error processing chunk {chunk_info}: {str(e)}')
            chunk_summaries.append(f"[{chunk_info}] ã‚¨ãƒ©ãƒ¼: {str(e)}")
    
    # ãƒãƒ£ãƒ³ã‚¯æ¦‚è¦ã‚’çµ±åˆ
    return "\n\n".join(chunk_summaries)


def create_sheet_summaries(basename: str, all_sheets: Dict[str, pd.DataFrame], 
                          client: AzureOpenAI, use_chunking: bool = False) -> Dict[str, str]:
    """
    å…¨ã‚·ãƒ¼ãƒˆã®æ¦‚è¦ã‚’ä½œæˆ
    
    Args:
        basename: ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ™ãƒ¼ã‚¹å
        all_sheets: {ã‚·ãƒ¼ãƒˆå: DataFrame}ã®è¾æ›¸
        client: AzureOpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
        use_chunking: ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ã‚’ä½¿ç”¨ã™ã‚‹ã‹
    
    Returns:
        {ã‚·ãƒ¼ãƒˆå: æ¦‚è¦}ã®è¾æ›¸
    """
    sheet_summaries = {}
    
    for i, (sheet_name, df) in enumerate(all_sheets.items(), 1):
        print(f'  Summarizing sheet {i}/{len(all_sheets)}: {sheet_name} ({len(df)} rows)')
        
        try:
            summary = summarize_sheet(basename, sheet_name, df, client, use_chunking)
            sheet_summaries[sheet_name] = summary
            
            # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–
            if i < len(all_sheets):
                time.sleep(WAIT_TIME_BETWEEN_SHEETS)
                
        except Exception as e:
            print(f'  âš  Error summarizing sheet {sheet_name}: {str(e)}')
            sheet_summaries[sheet_name] = f"ã‚¨ãƒ©ãƒ¼: {str(e)}"
    
    return sheet_summaries


def create_final_summary(basename: str, sheet_summaries: Dict[str, str], 
                        client: AzureOpenAI) -> str:
    """
    å…¨ã‚·ãƒ¼ãƒˆæ¦‚è¦ã‹ã‚‰æœ€çµ‚çš„ãªè§£èª¬ã‚’ä½œæˆ
    
    Args:
        basename: ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ™ãƒ¼ã‚¹å
        sheet_summaries: {ã‚·ãƒ¼ãƒˆå: æ¦‚è¦}ã®è¾æ›¸
        client: AzureOpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
    
    Returns:
        æœ€çµ‚çš„ãªè§£èª¬ï¼ˆMarkdownå½¢å¼ï¼‰
    """
    # å…¨ã‚·ãƒ¼ãƒˆæ¦‚è¦ã‚’ãƒ†ã‚­ã‚¹ãƒˆã«æ•´å½¢
    summaries_text = []
    for sheet_name, summary in sheet_summaries.items():
        summaries_text.append(f"### {sheet_name}\n{summary}\n")
    
    combined_summaries = "\n".join(summaries_text)
    
    response = client.chat.completions.create(
        model=DEPLOYMENT_NAME,
        messages=[
            {"role": "system", "content": FINAL_SUMMARY_PROMPT.format(basename=basename)},
            {"role": "user", "content": combined_summaries}
        ],
        max_tokens=10000
    )
    return response.choices[0].message.content


def create_summary(rel_path: str, resource_dir: str, client: AzureOpenAI, 
                  use_chunking: bool = False) -> str:
    """
    Excelãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰è§£èª¬ã‚’ä½œæˆï¼ˆ2æ®µéšæ–¹å¼ï¼‰
    
    æ®µéš1: å„ã‚·ãƒ¼ãƒˆã®æ¦‚è¦ã‚’ä½œæˆ
    æ®µéš2: å…¨ã‚·ãƒ¼ãƒˆæ¦‚è¦ã‚’çµ±åˆã—ã¦æœ€çµ‚çš„ãªè§£èª¬ã‚’ä½œæˆ
    
    Args:
        rel_path: resourceãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰ã®ç›¸å¯¾ãƒ‘ã‚¹ï¼ˆæ‹¡å¼µå­ãªã—ï¼‰
        resource_dir: resourceãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹
        client: AzureOpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
        use_chunking: ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ã‚’ä½¿ç”¨ã™ã‚‹ã‹
    
    Returns:
        æœ€çµ‚çš„ãªè§£èª¬ï¼ˆMarkdownå½¢å¼ï¼‰
    """
    basename = os.path.basename(rel_path)
    
    # Excelãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ•ãƒ«ãƒ‘ã‚¹ã‚’æ§‹ç¯‰
    excel_path = os.path.join(resource_dir, f'{rel_path}.xlsx')
    if not os.path.exists(excel_path):
        excel_path = os.path.join(resource_dir, f'{rel_path}.xls')
    
    # å…¨ã‚·ãƒ¼ãƒˆã‚’èª­ã¿è¾¼ã‚€
    all_sheets = pd.read_excel(excel_path, sheet_name=None)
    print(f'  Found {len(all_sheets)} sheets')
    
    # æ®µéš1: å„ã‚·ãƒ¼ãƒˆã®æ¦‚è¦ã‚’ä½œæˆ
    print(f'  Phase 1: Creating sheet summaries...')
    sheet_summaries = create_sheet_summaries(basename, all_sheets, client, use_chunking)
    
    # æ®µéš2: å…¨ä½“ã®è§£èª¬ã‚’ä½œæˆ
    print(f'  Phase 2: Creating final summary...')
    time.sleep(WAIT_TIME_BETWEEN_SHEETS)
    final_summary = create_final_summary(basename, sheet_summaries, client)
    
    # æœ€çµ‚çš„ãªMarkdownã‚’æ§‹ç¯‰
    result_parts = [
        f"# {basename} - æ©Ÿèƒ½è§£èª¬\n\n",
        final_summary,
        "\n\n---\n\n",
        "## å„ã‚·ãƒ¼ãƒˆæ¦‚è¦\n\n"
    ]
    
    for sheet_name, summary in sheet_summaries.items():
        result_parts.append(f"### {sheet_name}\n\n{summary}\n\n")
    
    return "".join(result_parts)


def main(use_chunking: bool = False):
    """
    ãƒ¡ã‚¤ãƒ³å‡¦ç†
    
    Args:
        use_chunking: å¤§ããªã‚·ãƒ¼ãƒˆã‚’ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ã™ã‚‹ã‹
    """
    inputs = get_excel_files(INPUT_DIR)
    
    # å‡ºåŠ›æ¸ˆã¿ã®ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—
    output_mds = []
    if os.path.exists(OUTPUT_DIR):
        for root, dirs, files in os.walk(OUTPUT_DIR):
            for file in files:
                if file.endswith('.md'):
                    full_path = os.path.join(root, file)
                    rel_path = os.path.relpath(full_path, OUTPUT_DIR)
                    rel_path_without_ext = os.path.splitext(rel_path)[0]
                    output_mds.append(rel_path_without_ext)
    
    # æœªå‡¦ç†ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æŠ½å‡º
    undescribeds = [f for f in inputs if f not in output_mds]
    
    print(f'Found {len(inputs)} Excel files')
    print(f'Already processed: {len(output_mds)}')
    print(f'Processing {len(undescribeds)} files...')
    print(f'Chunking mode: {"ON" if use_chunking else "OFF"}')
    
    if len(undescribeds) == 0:
        print('No files to process.')
        return
    
    client = get_client()
    
    for idx, undescribed in enumerate(undescribeds, 1):
        try:
            print(f'\n[{idx}/{len(undescribeds)}] ğŸ“„ Processing: {undescribed}')
            description = create_summary(undescribed, INPUT_DIR, client, use_chunking)
            
            # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’æ§‹ç¯‰
            output_path = os.path.join(OUTPUT_DIR, f'{undescribed}.md')
            
            # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆï¼ˆå­˜åœ¨ã—ãªã„å ´åˆï¼‰
            output_dir = os.path.dirname(output_path)
            if output_dir and not os.path.exists(output_dir):
                os.makedirs(output_dir, exist_ok=True)
            
            # ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(description)
            
            print(f'âœ“ Summary file created: {undescribed}')
            
            # ãƒ•ã‚¡ã‚¤ãƒ«é–“ã®å¾…æ©Ÿ
            if idx < len(undescribeds):
                time.sleep(WAIT_TIME_BETWEEN_FILES)
            
        except Exception as e:
            print(f'âœ— Error processing {undescribed}: {str(e)}')
    
    print(f'\n{"="*60}')
    print(f'Processing complete!')


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='Create summary from Excel files')
    parser.add_argument('--chunking', action='store_true', 
                       help='Enable chunking for large sheets')
    
    args = parser.parse_args()
    
    main(use_chunking=args.chunking)
